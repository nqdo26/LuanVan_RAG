from fastapi import FastAPI, Request, HTTPException
from pydantic import BaseModel
import requests
from pinecone import Pinecone, ServerlessSpec
from groq import Groq
from langchain.text_splitter import RecursiveCharacterTextSplitter
import re
import unicodedata
import os
from dotenv import load_dotenv

from models import IngestPayload, QuestionPayload, DeletePayload, ChatCompletionPayload

# Load environment variables
load_dotenv()
app = FastAPI()

# Pinecone init
pc = Pinecone(
    api_key=os.getenv("PINECONE_API_KEY")
)

index_name = os.getenv("PINECONE_INDEX_NAME")

# Groq init
client = Groq(
    api_key=os.environ.get("GROQ_API_KEY"),
)

# Define the text splitter
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators=["\n\n", "\n", ".", " ", ""]
)

# Check if the index exists, if not create it
if not pc.has_index(index_name):
    pc.create_index_for_model(
        name=index_name,
        cloud="aws",
        region="us-east-1",
        embed={
            "model":"llama-text-embed-v2",
            "field_map":{"text": "text"}
        }
    )

# Connect to the index
index = pc.Index(index_name)


# Define endpoints
@app.get("/")
def hello_world():
    return {"message": "Hello World!"}

@app.head("/v1/keep-alive")
def health_check():
        return {"status": "healthy"}

@app.post("/v1/ingest")
def ingest(payload: IngestPayload):
        
        
        chunks = text_splitter.split_text(payload.info)


        # Add slug and name to each record for downstream use
        records = [
            {
                "id": f"{payload.destinationId}-{i}",
                "text": chunk,
                'destinationId': payload.destinationId,
                'slug': getattr(payload, 'slug', None),
                'name': getattr(payload, 'name', None),
            } for i, chunk in enumerate(chunks)
        ]

        # Batch size of 90 (below Pinecone's limit of 96)
        batch_size = 90
    
        # Split records into batches and upsert
        for i in range(0, len(records), batch_size):
            batch = records[i:i + batch_size]
            index.upsert_records(payload.cityId, batch)

        return {"status": "done", "chunks_processed": len(records)}

@app.post("/v1/question")
def question(payload: QuestionPayload):
    # Define the query

    # Search the dense index
    results = index.search(
        namespace=payload.cityId,
        query={
            "top_k": 10,
            "inputs": {
                'text': payload.query
            }
        }
    )

    # Print the results
    for hit in results['result']['hits']:
            print(f"id: {hit['_id']:<5} | destinationId: {hit['fields']['destinationId']} | text: {hit['fields']['text']:<50}")
            

    chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": (
                "### üìò Y√™u c·∫ßu:\n"
                f"Tr·∫£ l·ªùi c√¢u h·ªèi sau b·∫±ng c√°ch d·ª±a tr√™n c√°c ƒëo·∫°n vƒÉn b√™n d∆∞·ªõi. "
                "N·∫øu th√¥ng tin kh√¥ng ƒë·ªß, h√£y tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c c·ªßa b·∫°n v√† ghi r√µ ƒëi·ªÅu ƒë√≥.\n\n"
                f"**C√¢u h·ªèi:** {payload.query}\n\n"
                "### üìö ƒêo·∫°n vƒÉn tham kh·∫£o:\n"
                # + "\n---\n".join([hit['fields']['text'] for hit in results['result']['hits']]) +
                # "\n\n"
                + "\n---\n".join([
                     f"**ƒêo·∫°n vƒÉn {i+1}:**\n"
                     f"{hit['fields']['text']}\n"
                     for i, hit in enumerate(results['result']['hits'])
                     ]) +
                "### ‚úèÔ∏è Ghi ch√∫ khi tr·∫£ l·ªùi:\n"
                "- Tr√¨nh b√†y c√¢u tr·∫£ l·ªùi b·∫±ng [Markdown] ƒë·ªÉ h·ªá th·ªëng `react-markdown` c√≥ th·ªÉ hi·ªÉn th·ªã t·ªët.\n"
                "- Th√™m emoji ph√π h·ª£p ƒë·ªÉ l√†m n·ªïi b·∫≠t n·ªôi dung ch√≠nh üß†üìåüí°.\n"
                "- N·∫øu c√¢u tr·∫£ l·ªùi kh√¥ng th·ªÉ r√∫t ra t·ª´ ƒëo·∫°n vƒÉn, h√£y b·∫Øt ƒë·∫ßu b·∫±ng c√¢u: `D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë g·ª£i √Ω c·ªßa t√¥i, ƒë·ªÉ c√≥ th·ªÉ nh·∫≠n g·ª£i √Ω ch√≠nh x√°c h∆°n t·ª´ h·ªá th·ªëng, vui l√≤ng ch·ªçn ƒëi·ªÉm ƒë·∫øn tr∆∞·ªõc nh√©`"
            )
        }
    ],
    model=payload.model or "deepseek-r1-distill-llama-70b",
    )
    response_dict = chat_completion.model_dump()
    response_dict["choices"][0]["message"]["documents"] = [
        {
            "id": hit["_id"],
            "text": hit["fields"]["text"],
            "destinationId": hit["fields"]["destinationId"],
            "score": hit["_score"]
        } for hit in results['result']['hits']
    ]
    return response_dict

@app.post("/v1/delete-document")
def delete_document(payload: DeletePayload):

    ids_to_delete = list(index.list(prefix=payload.destiationId, namespace=payload.cityId))

    if not ids_to_delete:
        raise HTTPException(status_code=404, detail="Kh√¥ng t√¨m th·∫•y vectors n√†o v·ªõi documentId n√†y.")

    # B∆∞·ªõc 2: Xo√° vector theo ID
    index.delete(
        namespace=payload.cityId,
        ids=ids_to_delete
    )

    return {"deleted_ids": ids_to_delete}

@app.post("/v1/chat/completions")
def create_chat_completion(payload: ChatCompletionPayload):
    """
    T·∫°o chat completion cho Gobot
    - B·∫°n l√† Gobot - tr·ª£ l√Ω du l·ªãch AI th√¥ng minh c·ªßa Vi·ªát Nam.
    - Ch·ªâ tr·∫£ l·ªùi c√°c ƒë·ªãa ƒëi·ªÉm du l·ªãch ·ªü Vi·ªát Nam.
    - N·∫øu kh√¥ng d√πng knowledge ho·∫∑c kh√¥ng c√≥ cityId -> tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c n·ªÅn
    - N·∫øu c√≥ knowledge -> t√¨m ki·∫øm Pinecone, AI tr·∫£ l·ªùi, link ƒë·ªãa ƒëi·ªÉm ƒë∆∞·ª£c gh√©p xu·ªëng cu·ªëi

    """
    notice = "üëÜ Nh·ªõ ch·ªçn ƒëi·ªÉm ƒë·∫øn ph√≠a tr√™n tr∆∞·ªõc khi h·ªèi ƒë·ªÉ Gobot g·ª£i √Ω ch√≠nh x√°c t·ª´ h·ªá th·ªëng nheee!"

    # -----------------------
    # 1Ô∏è‚É£ Tr∆∞·ªùng h·ª£p kh√¥ng d√πng knowledge
    # -----------------------
    if not payload.isUseKnowledge or not payload.cityId:
        try:
            messages_for_api = [msg.model_dump() for msg in payload.messages]
            last_message = messages_for_api[-1] if messages_for_api else None

            system_prompt = (
                "üåè Xin ch√†o!\n"
                f"B·∫°n h·ªèi: {last_message['content'] if last_message else ''}\n\n"
                "ü§ñ ƒê√¢y l√† c√¢u tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c n·ªÅn c·ªßa h·ªá th·ªëng.\n"
                "N·∫øu mu·ªën nh·∫≠n g·ª£i √Ω ch√≠nh x√°c h∆°n, h√£y ch·ªçn ƒëi·ªÉm ƒë·∫øn tr∆∞·ªõc nh√©! üëÜ\n\n"
                "---\n### Tr·∫£ l·ªùi:"
            )

            # Model fallback strategy
            primary_model = payload.model or "deepseek-r1-distill-llama-70b"
            fallback_models = [primary_model, "llama-3.3-70b-versatile", "llama-3.1-8b-instant"]
            
            response_dict = None
            for attempt, current_model in enumerate(fallback_models):
                try:
                    print(f"[NO-KNOWLEDGE ATTEMPT {attempt + 1}] Trying model: {current_model}")
                    chat_completion = client.chat.completions.create(
                        messages=messages_for_api[:-1] + [{"role": "user", "content": system_prompt}],
                        model=current_model,
                        temperature=0.3,
                        top_p=0.9,
                        max_completion_tokens=1024,
                    )
                    response_dict = chat_completion.model_dump()
                    print(f"[NO-KNOWLEDGE SUCCESS] Model {current_model} worked!")
                    break
                except Exception as e:
                    error_msg = str(e).lower()
                    if any(keyword in error_msg for keyword in ['quota', 'rate limit', 'token']) and attempt < len(fallback_models) - 1:
                        print(f"[NO-KNOWLEDGE FALLBACK] {current_model} failed, trying next...")
                        continue
                    elif attempt == len(fallback_models) - 1:
                        raise e
            
            if response_dict is None:
                raise HTTPException(status_code=500, detail="All fallback models failed")
            
            # Lo·∫°i b·ªè ph·∫ßn thinking c·ªßa deepseek model
            if response_dict.get("choices") and response_dict["choices"][0].get("message"):
                content = response_dict["choices"][0]["message"].get("content", "")
                # Lo·∫°i b·ªè ph·∫ßn thinking
                import re
                content = re.sub(r'<thinking>.*?</thinking>', '', content, flags=re.DOTALL)
                content = re.sub(r'^.*?(?=(?:Xin ch√†o|Ch√†o b·∫°n|D∆∞·ªõi ƒë√¢y|‚ö†Ô∏è|M√¨nh|T√¥i|B·∫°n))', '', content, flags=re.DOTALL)
                content = re.sub(r'Alright.*?(?=\n\n|\n[A-Z])', '', content, flags=re.DOTALL)
                content = re.sub(r'First.*?(?=\n\n|\n[A-Z])', '', content, flags=re.DOTALL)
                content = re.sub(r'I need.*?(?=\n\n|\n[A-Z])', '', content, flags=re.DOTALL)
                content = re.sub(r'The user.*?(?=\n\n|\n[A-Z])', '', content, flags=re.DOTALL)
                content = re.sub(r'\n\s*\n\s*\n', '\n\n', content)
                content = content.strip()
                
                # Gh√©p notice v√†o ƒë·∫ßu c√¢u tr·∫£ l·ªùi
                response_dict["choices"][0]["message"]["content"] = notice + "\n\n" + content
            return response_dict

        except Exception as e:
            raise HTTPException(status_code=500, detail=f"Error during chat completion: {str(e)}")

    # -----------------------
    # 2Ô∏è‚É£ Tr∆∞·ªùng h·ª£p d√πng knowledge
    # -----------------------
    messages_for_api = [msg.model_dump() for msg in payload.messages]

    # L√†m s·∫°ch c√¢u h·ªèi ƒë·ªÉ search Pinecone
    combined_question = " ".join(
        [msg.content for msg in payload.messages if msg.role == "user"]
    )
    combined_question = clean_text(combined_question)

    # T√¨m ki·∫øm nhi·ªÅu k·∫øt qu·∫£ ƒë·ªÉ AI c√≥ context ƒë·∫ßy ƒë·ªß
    results = index.search(
        namespace=payload.cityId,
        query={"top_k": 10, "inputs": {"text": combined_question}},
    )
    hits = results.get("result", {}).get("hits", [])

    # L·ªçc theo lo·∫°i h√¨nh n·∫øu c√¢u h·ªèi li√™n quan ƒë·∫øn c√† ph√™
    user_question = payload.messages[-1].content.lower()
    cafe_keywords = ["c√† ph√™", "coffee", "qu√°n cafe", "qu√°n c√† ph√™", "qu√°n c√† ph√™ 24h", "cafe"]
    if any(kw in user_question for kw in cafe_keywords):
        filtered_hits = []
        for hit in hits:
            # N·∫øu d·ªØ li·ªáu c√≥ tr∆∞·ªùng 'type' ho·∫∑c 'category', l·ªçc theo ƒë√≥
            type_val = (hit["fields"].get("type") or hit["fields"].get("category") or "").lower()
            name_val = (hit["fields"].get("name") or "").lower()
            # ∆Øu ti√™n type/category l√† cafe, ho·∫∑c t√™n c√≥ ch·ª©a t·ª´ cafe/c√† ph√™
            if "cafe" in type_val or "c√† ph√™" in type_val or "coffee" in type_val or "cafe" in name_val or "c√† ph√™" in name_val or "coffee" in name_val:
                filtered_hits.append(hit)
        # N·∫øu c√≥ k·∫øt qu·∫£ l·ªçc, d√πng k·∫øt qu·∫£ n√†y, n·∫øu kh√¥ng th√¨ fallback v·ªÅ hits ban ƒë·∫ßu
        if filtered_hits:
            hits = filtered_hits

    if not hits:
        return {
            "choices": [
                {"message": {"content": notice + "\n\n‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ph√π h·ª£p."}}
            ]
        }

    # -----------------------
    # 3Ô∏è‚É£ Chu·∫©n b·ªã danh s√°ch link g·ªçn g√†ng
    # -----------------------
    MAX_LINKS = 5
    seen = set()
    link_lines = []

    for hit in hits:
        slug = hit["fields"].get("slug")
        dest_id = hit["fields"].get("destinationId")
        name = hit["fields"].get("name") or f"ƒê·ªãa ƒëi·ªÉm {dest_id}"
        unique_key = slug or dest_id
        if unique_key and unique_key not in seen:
            seen.add(unique_key)
            url = f"http://localhost:3000/destination/{slug or dest_id}"
            link_lines.append(f"- [{name}]({url})")
        if len(link_lines) >= MAX_LINKS:
            break

    # -----------------------
    # 4Ô∏è‚É£ Chu·∫©n b·ªã prompt cho AI
    # -----------------------
    reference_texts = "\n---\n".join(
        [hit["fields"]["text"] for hit in hits]
    )

    prompt = (
        "B·∫°n l√† Gobot - tr·ª£ l√Ω du l·ªãch AI th√¥ng minh c·ªßa Vi·ªát Nam. "
        "H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin ƒë∆∞·ª£c cung c·∫•p b√™n d∆∞·ªõi.\n\n"
        
        f"üîç **C√¢u h·ªèi:** {payload.messages[-1].content}\n\n"
        
        "ÔøΩ **Th√¥ng tin t·ª´ c∆° s·ªü d·ªØ li·ªáu:**\n"
        f"{reference_texts}\n\n"
        
        "üìù **Y√™u c·∫ßu tr·∫£ l·ªùi:**\n"
        "‚Ä¢ Tr·∫£ l·ªùi ho√†n to√†n b·∫±ng ti·∫øng Vi·ªát, t·ª± nhi√™n v√† th√¢n thi·ªán\n"
        "‚Ä¢ D·ª±a v√†o th√¥ng tin tr√™n, ƒë∆∞a ra g·ª£i √Ω c·ª• th·ªÉ v√† h·ªØu √≠ch\n"
        "- C√¢u tr·∫£ l·ªùi ph·∫£i t·∫≠p trung v√†o c√°c ƒë·ªãa ƒëi·ªÉm du l·ªãch, ph·∫ßn suy nghƒ© c·ªßa b·∫°n kh√¥ng qu√° d√†i ƒë·ªÉ tr√°nh tr∆∞·ªùng h·ª£p lan mang"
        "‚Ä¢ M√¥ t·∫£ chi ti·∫øt v·ªÅ t·ª´ng ƒë·ªãa ƒëi·ªÉm: ƒë·ªãa ch·ªâ, gi√° c·∫£, ƒë·∫∑c ƒëi·ªÉm n·ªïi b·∫≠t\n"
        "‚Ä¢ S·∫Øp x·∫øp theo th·ª© t·ª± ∆∞u ti√™n (g·ª£i √Ω t·ªët nh·∫•t tr∆∞·ªõc)\n"
        "‚Ä¢ Th√™m emoji ph√π h·ª£p: ÔøΩÔ∏èüèîÔ∏èüçú‚òïüéØüìç\n"
        "‚Ä¢ K·∫øt th√∫c b·∫±ng l·ªùi khuy√™n ho·∫∑c m·∫πo du l·ªãch th·ª±c t·∫ø\n"
        "‚Ä¢ KH√îNG t·ª± th√™m link v√†o n·ªôi dung\n\n"
        
        "‚ö†Ô∏è **L∆∞u √Ω:** N·∫øu th√¥ng tin kh√¥ng ƒë·ªß chi ti·∫øt, h√£y b·∫Øt ƒë·∫ßu c√¢u tr·∫£ l·ªùi b·∫±ng: "
        "'‚ö†Ô∏è D·ª±a tr√™n d·ªØ li·ªáu hi·ªán c√≥, ƒë√¢y l√† nh·ªØng g·ª£i √Ω t·ªët nh·∫•t:'\n\n"
        
        "üí¨ **B·∫Øt ƒë·∫ßu tr·∫£ l·ªùi:**"
    )


    # -----------------------
    # 5Ô∏è‚É£ G·ªçi model v·ªõi fallback strategy
    # -----------------------
    model_used = payload.model or "deepseek-r1-distill-llama-70b"
    print(f"[GROQ MODEL] Using model: {model_used}")
    
    # Danh s√°ch fallback models
    fallback_models = [
        model_used,
        "llama-3.3-70b-versatile",
        "llama-3.1-8b-instant",
        "mixtral-8x7b-32768"
    ]
    
    response_dict = None
    last_error = None
    
    for attempt, current_model in enumerate(fallback_models):
        try:
            print(f"[ATTEMPT {attempt + 1}] Trying model: {current_model}")
            chat_completion = client.chat.completions.create(
                messages=messages_for_api + [{"role": "user", "content": prompt}],
                model=current_model,
                temperature=0.3,
                top_p=0.9,
                max_completion_tokens=1024,
            )
            response_dict = chat_completion.model_dump()
            print(f"[SUCCESS] Model {current_model} worked!")
            break
            
        except Exception as e:
            last_error = e
            error_msg = str(e).lower()
            print(f"[ERROR] Model {current_model} failed: {str(e)}")
            
            # Ki·ªÉm tra c√°c l·ªói li√™n quan ƒë·∫øn token/quota
            if any(keyword in error_msg for keyword in ['quota', 'rate limit', 'token', 'limit exceeded', '429']):
                print(f"[FALLBACK] Token/quota issue with {current_model}, trying next model...")
                continue
            else:
                # L·ªói kh√°c, c√≥ th·ªÉ th·ª≠ model kh√°c ho·∫∑c raise
                if attempt < len(fallback_models) - 1:
                    print(f"[FALLBACK] Other error with {current_model}, trying next model...")
                    continue
                else:
                    raise e
    
    if response_dict is None:
        raise HTTPException(status_code=500, detail=f"All models failed. Last error: {str(last_error)}")

    # Lo·∫°i b·ªè ph·∫ßn thinking c·ªßa deepseek model
    if response_dict.get("choices") and response_dict["choices"][-1].get("message"):
        content = response_dict["choices"][-1]["message"].get("content", "")
        # Lo·∫°i b·ªè ph·∫ßn thinking (th∆∞·ªùng b·∫Øt ƒë·∫ßu v·ªõi c√°c pattern n√†y)
        import re
        # Pattern 1: Lo·∫°i b·ªè text trong <thinking>...</thinking>
        content = re.sub(r'<thinking>.*?</thinking>', '', content, flags=re.DOTALL)
        # Pattern 2: Lo·∫°i b·ªè ph·∫ßn thinking ·ªü ƒë·∫ßu (th∆∞·ªùng l√† ti·∫øng Anh)
        content = re.sub(r'^.*?(?=(?:Xin ch√†o|Ch√†o b·∫°n|D∆∞·ªõi ƒë√¢y|‚ö†Ô∏è|M√¨nh|T√¥i|B·∫°n))', '', content, flags=re.DOTALL)
        # Pattern 3: Lo·∫°i b·ªè c√°c ƒëo·∫°n thinking kh√°c
        content = re.sub(r'Alright.*?(?=\n\n|\n[A-Z])', '', content, flags=re.DOTALL)
        content = re.sub(r'First.*?(?=\n\n|\n[A-Z])', '', content, flags=re.DOTALL)
        content = re.sub(r'I need.*?(?=\n\n|\n[A-Z])', '', content, flags=re.DOTALL)
        content = re.sub(r'The user.*?(?=\n\n|\n[A-Z])', '', content, flags=re.DOTALL)
        # Clean up extra whitespace
        content = re.sub(r'\n\s*\n\s*\n', '\n\n', content)
        content = content.strip()
        response_dict["choices"][-1]["message"]["content"] = content

    # -----------------------
    # 6Ô∏è‚É£ Gh√©p link th√¥ng minh v√†o cu·ªëi c√¢u tr·∫£ l·ªùi
    # -----------------------
    if response_dict.get("choices") and response_dict["choices"][-1].get("message"):
        content = response_dict["choices"][-1]["message"].get("content", "")
        
        # T·∫°o danh s√°ch ƒë·ªãa ƒëi·ªÉm ƒë∆∞·ª£c ƒë·ªÅ c·∫≠p
        mentioned_places = []
        seen_destinations = set()
        
        for hit in hits:
            dest_name = hit["fields"].get("name", "")
            dest_slug = hit["fields"].get("slug", "")
            dest_id = hit["fields"].get("destinationId", "")
            
            # Ki·ªÉm tra xem ƒë·ªãa ƒëi·ªÉm c√≥ ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn trong c√¢u tr·∫£ l·ªùi kh√¥ng
            name_in_content = dest_name.lower() in content.lower() if dest_name else False
            slug_in_content = dest_slug.lower() in content.lower() if dest_slug else False
            
            if (name_in_content or slug_in_content) and dest_id not in seen_destinations:
                seen_destinations.add(dest_id)
                link_slug = dest_slug if dest_slug else dest_id
                place_name = dest_name if dest_name else f"ƒê·ªãa ƒëi·ªÉm {dest_id}"
                url = f"http://localhost:3000/destination/{link_slug}"
                mentioned_places.append(f"üîó [{place_name}]({url})")
        
        # N·∫øu kh√¥ng c√≥ ƒë·ªãa ƒëi·ªÉm n√†o ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn c·ª• th·ªÉ, l·∫•y top 3 k·∫øt qu·∫£ t·ªët nh·∫•t
        if not mentioned_places:
            for i, hit in enumerate(hits[:3]):
                dest_name = hit["fields"].get("name", f"ƒê·ªãa ƒëi·ªÉm {hit['fields'].get('destinationId', '')}")
                dest_slug = hit["fields"].get("slug", hit["fields"].get("destinationId", ""))
                url = f"http://localhost:3000/destination/{dest_slug}"
                mentioned_places.append(f"üîó [{dest_name}]({url})")
        
        # Th√™m section ƒë∆∞·ªùng d·∫´n n·∫øu c√≥
        if mentioned_places:
            links_section = f"\n\n---\n**ÔøΩ Kh√°m ph√° chi ti·∫øt:**\n" + "\n".join(mentioned_places)
            response_dict["choices"][-1]["message"]["content"] = content + links_section

    # -----------------------
    # 7Ô∏è‚É£ G·∫Øn danh s√°ch ƒë·ªãa ƒëi·ªÉm v√†o JSON tr·∫£ v·ªÅ
    # -----------------------
    response_dict["choices"][-1]["message"]["destinations"] = [
        {
            "id": hit["_id"],
            "text": hit["fields"]["text"],
            "destinationId": hit["fields"].get("destinationId"),
            "slug": hit["fields"].get("slug"),
            "name": hit["fields"].get("name"),
            "score": hit["_score"],
        } for hit in hits
    ]

    return response_dict

# ƒê∆∞a h√†m clean_text ra ngo√†i
def clean_text(text: str) -> str:
    # 1. Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng
    # text = text.lower()

    # 2. Chu·∫©n h√≥a Unicode (d√πng NFC ƒë·ªÉ gh√©p d·∫•u)
    text = unicodedata.normalize("NFC", text)

    # 3. Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát (gi·ªØ l·∫°i ti·∫øng Vi·ªát v√† ch·ªØ s·ªë)
    text = re.sub(r"[^\w\s√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©"
                r"√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë]", "", text)

    # 4. Lo·∫°i b·ªè kho·∫£ng tr·∫Øng d∆∞ th·ª´a
    text = re.sub(r"\s+", " ", text).strip()

    return text
    


    