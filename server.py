from fastapi import FastAPI, Request, HTTPException
from fastapi.exceptions import RequestValidationError
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import requests
from pinecone import Pinecone, ServerlessSpec
from groq import Groq
from langchain.text_splitter import RecursiveCharacterTextSplitter
import re
import unicodedata
import os
from dotenv import load_dotenv

from models import IngestPayload, QuestionPayload, DeletePayload, ChatCompletionPayload, UpdatePayload

# Load environment variables
load_dotenv()
app = FastAPI()

# ThÃªm exception handler cho validation errors
@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    print(f"[VALIDATION ERROR] {exc.errors()}")
    print(f"[REQUEST BODY] {await request.body()}")
    return JSONResponse(
        status_code=422,
        content={"detail": exc.errors(), "body": str(await request.body())}
    )

# Pinecone init
pc = Pinecone(
    api_key=os.getenv("PINECONE_API_KEY")
)

index_name = os.getenv("PINECONE_INDEX_NAME")

# Groq init
client = Groq(
    api_key=os.environ.get("GROQ_API_KEY"),
)

# Define the text splitter vá»›i semantic separators cho du lá»‹ch
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,  # TÄƒng size Ä‘á»ƒ giá»¯ nguyÃªn thÃ´ng tin
    chunk_overlap=100,  # TÄƒng overlap Ä‘á»ƒ giá»¯ context
    separators=[
        "\n### ",  # PhÃ¢n chia theo section headers
        "\n## ",   # Headers nhá» hÆ¡n
        "\n- ",    # List items
        ".\n",     # Káº¿t thÃºc cÃ¢u + newline
        ". ",      # Káº¿t thÃºc cÃ¢u
        "\n",      # Newline
        " "        # Space (fallback)
    ]
)

# Check if the index exists, if not create it
if not pc.has_index(index_name):
    pc.create_index_for_model(
        name=index_name,
        cloud="aws",
        region="us-east-1",
        embed={
            "model":"llama-text-embed-v2",
            "field_map":{"text": "text"}
        }
    )

# Connect to the index
index = pc.Index(index_name)


# Define endpoints
@app.get("/")
def hello_world():
    return {"message": "Hello World!"}

@app.head("/v1/keep-alive")
def health_check():
        return {"status": "healthy"}

@app.post("/v1/ingest")
def ingest(payload: IngestPayload):
        
        # Parse JSON data tá»« backend
        import json
        try:
            destination_data = json.loads(payload.info)
        except:
            # Fallback náº¿u váº«n lÃ  string cÅ©
            destination_data = {"description": payload.info}
        
        # Semantic chunking theo loáº¡i destination
        chunks = create_semantic_chunks(payload.name, destination_data, payload.destinationId, payload.slug)

        # Táº¡o records vá»›i metadata Ä‘áº§y Ä‘á»§
        records = [
            {
                "id": f"{payload.destinationId}-{chunk['type']}",
                "text": chunk['content'],
                'destinationId': payload.destinationId,
                'slug': payload.slug,
                'name': payload.name,
                'chunk_type': chunk['type'],
                'chunk_index': i,
                'total_chunks': len(chunks)
            } for i, chunk in enumerate(chunks)
        ]

        # Batch size of 90 (below Pinecone's limit of 96)
        batch_size = 90
    
        # Split records into batches and upsert
        for i in range(0, len(records), batch_size):
            batch = records[i:i + batch_size]
            index.upsert_records(payload.cityId, batch)

        return {"status": "done", "chunks_processed": len(records)}

@app.post("/v1/update")
def update_destination(payload: UpdatePayload):
    try:

        
        # BÆ°á»›c 1: TÃ¬m vÃ  xÃ³a cÃ¡c chunks cÅ© tá»« Táº¤T Cáº¢ namespaces
        # VÃ¬ cÃ³ thá»ƒ cityId Ä‘Ã£ thay Ä‘á»•i, ta cáº§n tÃ¬m trong táº¥t cáº£ namespaces
        deleted_count = 0
        
        # Láº¥y danh sÃ¡ch táº¥t cáº£ namespaces
        try:
            stats = index.describe_index_stats()
            all_namespaces = list(stats.get('namespaces', {}).keys())
            
            # TÃ¬m vÃ  xÃ³a chunks cÅ© trong táº¥t cáº£ namespaces
            for namespace in all_namespaces:
                try:
                    ids_to_delete = list(index.list(prefix=payload.destinationId, namespace=namespace))
                    if ids_to_delete:
                        index.delete(namespace=namespace, ids=ids_to_delete)
                        deleted_count += len(ids_to_delete)
                except Exception as ns_error:
                    continue
                    
        except Exception as stats_error:
            # Fallback: chá»‰ xÃ³a tá»« namespace hiá»‡n táº¡i
            try:
                ids_to_delete = list(index.list(prefix=payload.destinationId, namespace=payload.cityId))
                if ids_to_delete:
                    index.delete(namespace=payload.cityId, ids=ids_to_delete)
                    deleted_count = len(ids_to_delete)
            except Exception as fallback_error:
                pass
        
        # BÆ°á»›c 2: Parse JSON data tá»« backend (giá»‘ng nhÆ° ingest)
        import json
        try:
            destination_data = json.loads(payload.info)
            print(f"[UPDATE] ğŸ“ Äang update Ä‘á»‹a Ä‘iá»ƒm: {payload.name}")
        except Exception as parse_error:
            destination_data = {"description": payload.info}
        
        # BÆ°á»›c 3: Táº¡o semantic chunks má»›i vá»›i 4 chunks
        chunks = create_semantic_chunks(payload.name, destination_data, payload.destinationId, payload.slug)

        # BÆ°á»›c 4: Táº¡o records má»›i vá»›i metadata Ä‘áº§y Ä‘á»§
        records = [
            {
                "id": f"{payload.destinationId}-{chunk['type']}",
                "text": chunk['content'],
                'destinationId': payload.destinationId,
                'slug': payload.slug,
                'name': payload.name,
                'chunk_type': chunk['type'],
                'chunk_index': i,
                'total_chunks': len(chunks)
            } for i, chunk in enumerate(chunks)
        ]

        # BÆ°á»›c 5: Upsert records má»›i vÃ o namespace má»›i (cityId tá»« payload)
        batch_size = 90
        
        for i in range(0, len(records), batch_size):
            batch = records[i:i + batch_size]
            index.upsert_records(payload.cityId, batch)

        print(f"[UPDATE] âœ… UPDATE THÃ€NH CÃ”NG: {payload.name}")
        
        return {
            "status": "updated", 
            "chunks_deleted": deleted_count,
            "chunks_created": len(records),
            "new_namespace": payload.cityId,
            "destination_name": payload.name
        }
        
    except Exception as e:
        print(f"[UPDATE ERROR] âŒ UPDATE THáº¤T Báº I cho {payload.name}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Update failed: {str(e)}")

@app.post("/v1/question")
def question(payload: QuestionPayload):
    # Define the query

    # Search the dense index vá»›i tÄƒng top_k Ä‘á»ƒ bao phá»§ tá»‘t hÆ¡n
    results = index.search(
        namespace=payload.cityId,
        query={
            "top_k": 12,  # TÄƒng lÃªn vÃ¬ má»—i destination cÃ³ 4 chunks
            "inputs": {
                'text': payload.query
            }
        }
    )

    # Print the results vá»›i thÃ´ng tin chunk type
    for hit in results['result']['hits']:
            chunk_type = hit['fields'].get('chunk_type', 'unknown')
            print(f"id: {hit['_id']:<5} | type: {chunk_type:<10} | destinationId: {hit['fields']['destinationId']} | text: {hit['fields']['text'][:50]}")
            

    chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": (
                "### ğŸ“˜ YÃªu cáº§u:\n"
                f"Tráº£ lá»i cÃ¢u há»i sau báº±ng cÃ¡ch dá»±a trÃªn cÃ¡c Ä‘oáº¡n vÄƒn bÃªn dÆ°á»›i. "
                "Náº¿u thÃ´ng tin khÃ´ng Ä‘á»§, hÃ£y tráº£ lá»i dá»±a trÃªn kiáº¿n thá»©c cá»§a báº¡n vÃ  ghi rÃµ Ä‘iá»u Ä‘Ã³.\n\n"
                f"**CÃ¢u há»i:** {payload.query}\n\n"
                "### ğŸ“š Äoáº¡n vÄƒn tham kháº£o:\n"
                # + "\n---\n".join([hit['fields']['text'] for hit in results['result']['hits']]) +
                # "\n\n"
                + "\n---\n".join([
                     f"**Äoáº¡n vÄƒn {i+1}:**\n"
                     f"{hit['fields']['text']}\n"
                     for i, hit in enumerate(results['result']['hits'])
                     ]) +
                "### âœï¸ Ghi chÃº khi tráº£ lá»i:\n"
                "- TrÃ¬nh bÃ y cÃ¢u tráº£ lá»i báº±ng [Markdown] Ä‘á»ƒ há»‡ thá»‘ng `react-markdown` cÃ³ thá»ƒ hiá»ƒn thá»‹ tá»‘t.\n"
                "- ThÃªm emoji phÃ¹ há»£p Ä‘á»ƒ lÃ m ná»•i báº­t ná»™i dung chÃ­nh ğŸ§ ğŸ“ŒğŸ’¡.\n"
                "- Náº¿u cÃ¢u tráº£ lá»i khÃ´ng thá»ƒ rÃºt ra tá»« Ä‘oáº¡n vÄƒn, hÃ£y báº¯t Ä‘áº§u báº±ng cÃ¢u: `DÆ°á»›i Ä‘Ã¢y lÃ  má»™t sá»‘ gá»£i Ã½ cá»§a tÃ´i, Ä‘á»ƒ cÃ³ thá»ƒ nháº­n gá»£i Ã½ chÃ­nh xÃ¡c hÆ¡n tá»« há»‡ thá»‘ng, vui lÃ²ng chá»n Ä‘iá»ƒm Ä‘áº¿n trÆ°á»›c nhÃ©`"
            )
        }
    ],
    model=payload.model or "deepseek-r1-distill-llama-70b",
    )
    response_dict = chat_completion.model_dump()
    response_dict["choices"][0]["message"]["documents"] = [
        {
            "id": hit["_id"],
            "text": hit["fields"]["text"],
            "destinationId": hit["fields"]["destinationId"],
            "score": hit["_score"]
        } for hit in results['result']['hits']
    ]
    return response_dict

@app.post("/v1/delete-document")
def delete_document(payload: DeletePayload):

    ids_to_delete = list(index.list(prefix=payload.destiationId, namespace=payload.cityId))

    if not ids_to_delete:
        raise HTTPException(status_code=404, detail="KhÃ´ng tÃ¬m tháº¥y vectors nÃ o vá»›i documentId nÃ y.")

    # BÆ°á»›c 2: XoÃ¡ vector theo ID
    index.delete(
        namespace=payload.cityId,
        ids=ids_to_delete
    )

    return {"deleted_ids": ids_to_delete}

# HÃ m clean_text Ä‘á»ƒ xá»­ lÃ½ vÄƒn báº£n
def clean_text(text: str) -> str:
    """
    LÃ m sáº¡ch vÃ  chuáº©n hÃ³a vÄƒn báº£n tiáº¿ng Viá»‡t
    """
    # 1. Chuáº©n hÃ³a Unicode (dÃ¹ng NFC Ä‘á»ƒ ghÃ©p dáº¥u)
    text = unicodedata.normalize("NFC", text)

    # 2. Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t (giá»¯ láº¡i tiáº¿ng Viá»‡t vÃ  chá»¯ sá»‘)
    text = re.sub(r"[^\w\sÃ Ã¡áº¡áº£Ã£Ã¢áº§áº¥áº­áº©áº«Äƒáº±áº¯áº·áº³áºµÃ¨Ã©áº¹áº»áº½Ãªá»áº¿á»‡á»ƒá»…Ã¬Ã­á»‹á»‰Ä©"
                r"Ã²Ã³á»á»ÃµÃ´á»“á»‘á»™á»•á»—Æ¡á»á»›á»£á»Ÿá»¡Ã¹Ãºá»¥á»§Å©Æ°á»«á»©á»±á»­á»¯á»³Ã½á»µá»·á»¹Ä‘]", "", text)

    # 3. Loáº¡i bá» khoáº£ng tráº¯ng dÆ° thá»«a
    text = re.sub(r"\s+", " ", text).strip()

    return text

def create_semantic_chunks(name: str, data: dict, destination_id: str, slug: str = None) -> list:
    """
    Táº¡o 4 chunks semantic cho má»—i destination theo strategy má»›i
    """
    chunks = []
    
    # 1. Tá»•ng quan - ThÃ´ng tin tá»•ng quan (báº¯t Ä‘áº§u báº±ng tÃªn Ä‘á»‹a Ä‘iá»ƒm)
    overview_content = f"**{name}**\n\n"
    
    # MÃ´ táº£ chÃ­nh
    if data.get('description'):
        overview_content += f"{data['description']}\n\n"
    
    # Äiá»ƒm ná»•i báº­t
    if data.get('highlight'):
        overview_content += f"**Äiá»ƒm ná»•i báº­t:** {data['highlight']}\n\n"
    
    chunks.append({
        'type': 'tong-quan',
        'content': overview_content.strip()
    })
    
    # 2. Tráº£i nghiá»‡m - Tráº£i nghiá»‡m vÃ  hoáº¡t Ä‘á»™ng (báº¯t Ä‘áº§u báº±ng tÃªn Ä‘á»‹a Ä‘iá»ƒm)
    experience_content = f"**{name}**\n\n"
    
    # Dá»‹ch vá»¥
    if data.get('services'):
        experience_content += f"**Dá»‹ch vá»¥:** {data['services']}\n\n"
    
    # Hoáº¡t Ä‘á»™ng
    if data.get('activities'):
        experience_content += f"**Hoáº¡t Ä‘á»™ng:** {data['activities']}\n\n"
    
    # ThÃ´ng tin há»¯u Ã­ch
    if data.get('usefulInfo'):
        experience_content += f"**ThÃ´ng tin há»¯u Ã­ch:** {data['usefulInfo']}\n\n"
    
    chunks.append({
        'type': 'trai-nghiem',
        'content': experience_content.strip()
    })
    
    # 3. Thá»±c táº¿ - ThÃ´ng tin thá»±c táº¿ (báº¯t Ä‘áº§u báº±ng tÃªn Ä‘á»‹a Ä‘iá»ƒm)
    practical_content = f"**{name}**\n\n"
    
    # Giá» má»Ÿ cá»­a
    if data.get('openHour'):
        practical_content += f"**Giá» má»Ÿ cá»­a:** {data['openHour']}\n\n"
    
    # PhÃ­ tham quan
    if data.get('fee'):
        practical_content += f"**PhÃ­ tham quan:** {data['fee']}\n\n"
    
    # ThÃ´ng tin liÃªn há»‡
    if data.get('contactInfo'):
        practical_content += f"**LiÃªn há»‡:** {data['contactInfo']}\n\n"
    
    chunks.append({
        'type': 'thuc-te',
        'content': practical_content.strip()
    })
    
    # 4. Danh má»¥c - Tags vÃ  tá»« khÃ³a tÃ¬m kiáº¿m (báº¯t Ä‘áº§u báº±ng tÃªn Ä‘á»‹a Ä‘iá»ƒm)
    tags_content = f"**{name}**\n\n"
    
    # Tags chÃ­nh - lÃ m ná»•i báº­t Ä‘á»ƒ AI dá»… nháº­n biáº¿t
    if data.get('tags'):
        tags_content += f"ğŸ·ï¸ **DANH Má»¤C/TAGS:** {data['tags']}\n\n"
    
    # Loáº¡i hÃ¬nh du lá»‹ch - chuyá»ƒn tá»« chunk tá»•ng quan
    if data.get('cultureType'):
        tags_content += f"ğŸ¯ **LOáº I HÃŒNH DU Lá»ŠCH:** {data['cultureType']}\n\n"
    
    # ThÃªm thÃ´ng tin phÃ¢n loáº¡i Ä‘á»ƒ AI dá»… phÃ¢n tÃ­ch
    if data.get('type'):
        tags_content += f"ğŸ“‚ **PHÃ‚N LOáº I:** {data['type']}\n\n"

    chunks.append({
        'type': 'danh-muc',
        'content': tags_content.strip()
    })
    
    return chunks

def filter_destinations_by_content(content: str, hits: list) -> list:
    """
    Lá»c destinations dá»±a trÃªn viá»‡c tÃªn Ä‘á»‹a Ä‘iá»ƒm cÃ³ xuáº¥t hiá»‡n trong ná»™i dung cÃ¢u tráº£ lá»i hay khÃ´ng
    Sá»­ dá»¥ng multiple matching strategies Ä‘á»ƒ tÄƒng Ä‘á»™ chÃ­nh xÃ¡c
    """
    if not content or not hits:
        return hits
    
    # Chuáº©n hÃ³a content Ä‘á»ƒ so sÃ¡nh
    content_normalized = clean_text(content.lower())
    
    # Táº¡o set Ä‘á»ƒ trÃ¡nh trÃ¹ng láº·p destinations
    mentioned_destination_ids = set()
    filtered_hits = []
    
    for hit in hits:
        dest_name = hit["fields"].get("name", "").strip()
        dest_id = hit["fields"].get("destinationId")
        
        if not dest_name or not dest_id:
            continue
            
        # Chuáº©n hÃ³a tÃªn Ä‘á»‹a Ä‘iá»ƒm Ä‘á»ƒ so sÃ¡nh
        dest_name_normalized = clean_text(dest_name.lower())
        
        # Strategy 1: Exact match
        is_mentioned = dest_name_normalized in content_normalized
        
        # Strategy 2: Partial word match (trÃ¡nh false positive)
        if not is_mentioned:
            # TÃ¡ch tá»« vÃ  kiá»ƒm tra tá»«ng tá»« quan trá»ng
            dest_words = [word for word in dest_name_normalized.split() if len(word) > 2]
            if dest_words:
                # Pháº£i cÃ³ Ã­t nháº¥t 70% tá»« xuáº¥t hiá»‡n trong content
                matched_words = sum(1 for word in dest_words if word in content_normalized)
                word_match_ratio = matched_words / len(dest_words)
                is_mentioned = word_match_ratio >= 0.7
        
        # Strategy 3: Common abbreviations vÃ  alternative names
        if not is_mentioned:
            # Kiá»ƒm tra cÃ¡c pattern phá»• biáº¿n
            name_patterns = [
                dest_name_normalized.replace(" ", ""),  # Loáº¡i bá» khoáº£ng tráº¯ng
                dest_name_normalized.replace("quÃ¡n", "").strip(),  # Loáº¡i bá» "quÃ¡n"
                dest_name_normalized.replace("nhÃ  hÃ ng", "").strip(),  # Loáº¡i bá» "nhÃ  hÃ ng"
                dest_name_normalized.replace("khÃ¡ch sáº¡n", "").strip(),  # Loáº¡i bá» "khÃ¡ch sáº¡n"
                dest_name_normalized.replace("cÃ  phÃª", "coffee").strip(),  # Thay tháº¿ cÃ  phÃª
            ]
            
            for pattern in name_patterns:
                if pattern and len(pattern) > 2 and pattern in content_normalized:
                    is_mentioned = True
                    break
        
        if is_mentioned:
            # Chá»‰ thÃªm náº¿u chÆ°a cÃ³ destination nÃ y
            if dest_id not in mentioned_destination_ids:
                mentioned_destination_ids.add(dest_id)
                filtered_hits.append(hit)
                print(f"[FILTER] âœ… Giá»¯ láº¡i: {dest_name} (xuáº¥t hiá»‡n trong cÃ¢u tráº£ lá»i)")
            else:
                print(f"[FILTER] âš ï¸ Bá» qua duplicate: {dest_name}")
        else:
            print(f"[FILTER] âŒ Loáº¡i bá»: {dest_name} (khÃ´ng xuáº¥t hiá»‡n trong cÃ¢u tráº£ lá»i)")
    
    print(f"[FILTER] ğŸ“Š Káº¿t quáº£: {len(filtered_hits)}/{len(hits)} destinations Ä‘Æ°á»£c giá»¯ láº¡i")
    return filtered_hits

@app.post("/v1/chat/completions")
def create_chat_completion(payload: ChatCompletionPayload):
    """
    Chat completion cho Gobot - trá»£ lÃ½ du lá»‹ch Viá»‡t Nam
    """
    # ThÃ´ng bÃ¡o chung khi chÆ°a chá»n thÃ nh phá»‘
    notice = (
        "ğŸ‘‹ Xin chÃ o! Hiá»‡n táº¡i báº¡n **chÆ°a chá»n thÃ nh phá»‘** hoáº·c Ä‘iá»ƒm Ä‘áº¿n cá»¥ thá»ƒ.\n"
        "Äá»ƒ nháº­n gá»£i Ã½ **chÃ­nh xÃ¡c tá»« há»‡ thá»‘ng**, hÃ£y chá»n má»™t thÃ nh phá»‘ trÆ°á»›c nhÃ©! ğŸ™ï¸\n"
    )

    # -----------------------
    # 1ï¸âƒ£ KhÃ´ng dÃ¹ng Knowledge
    # -----------------------
    if not payload.isUseKnowledge or not payload.cityId:
        messages_for_api = [msg.model_dump() for msg in payload.messages]
        user_question = messages_for_api[-1]["content"] if messages_for_api else ""

        system_prompt = (
            "Báº¡n lÃ  **Gobot**, trá»£ lÃ½ du lá»‹ch thÃ¢n thiá»‡n cá»§a **GoOhNo** ğŸ‡»ğŸ‡³.\n"
            "TÃ­nh cÃ¡ch: Vui váº», gáº§n gÅ©i, tráº£ lá»i ngáº¯n gá»n vÃ  há»¯u Ã­ch.\n"
            "Quy táº¯c:\n"
            "1. Chá»‰ tÆ° váº¥n vá» du lá»‹ch Viá»‡t Nam.\n"
            "2. Tráº£ lá»i ngáº¯n gá»n, thÃ¢n thiá»‡n báº±ng tiáº¿ng Viá»‡t.\n"
            "3. DÃ¹ng emoji Ä‘á»ƒ sinh Ä‘á»™ng (ğŸ“ğŸ–ï¸â˜•ğŸœğŸ¯).\n"
            "4. Khuyáº¿n khÃ­ch ngÆ°á»i dÃ¹ng chá»n thÃ nh phá»‘ Ä‘á»ƒ cÃ³ gá»£i Ã½ chÃ­nh xÃ¡c hÆ¡n.\n"
            "5. Náº¿u khÃ´ng cháº¯c: *MÃ¬nh khÃ´ng cháº¯c vá» Ä‘iá»u nÃ y nhÃ©.*\n"
        )


        user_prompt = (
            f"\"CÃ¢u há»i: {user_question}\"\n"
            "\"Tráº£ lá»i ngáº¯n gá»n vÃ  thÃ¢n thiá»‡n vá» du lá»‹ch Viá»‡t Nam.\"\n"
            "\"DÃ¹ng 4-5 cÃ¢u ngáº¯n vá»›i emoji. KhÃ´ng cáº§n quÃ¡ chi tiáº¿t.\"\n"
            "\"Khuyáº¿n khÃ­ch chá»n thÃ nh phá»‘ cá»¥ thá»ƒ Ä‘á»ƒ cÃ³ gá»£i Ã½ chÃ­nh xÃ¡c hÆ¡n tá»« há»‡ thá»‘ng.\"\n"
            "\"Káº¿t thÃºc báº±ng cÃ¢u chÃºc vui váº».\"\n"
        )

        messages = [
            {"role": "system", "content": system_prompt},
            *messages_for_api,
            {"role": "user", "content": user_prompt},
        ]

        fallback_models = [
            payload.model or "openai/gpt-oss-120b",
            "llama-3.3-70b-versatile",
            "mixtral-8x7b-32768"
        ]

        response_dict = None
        last_error = None

        for attempt, model in enumerate(fallback_models):
            try:
                print(f"[NO-KNOWLEDGE ATTEMPT {attempt+1}] Model: {model}")
                chat_completion = client.chat.completions.create(
                    messages=messages,
                    model=model,
                    temperature=0.4,
                    top_p=0.9,
                    max_completion_tokens=512,  
                )
                response_dict = chat_completion.model_dump()
                print(f"[SUCCESS] Model {model} worked!")
                break
            except Exception as e:
                last_error = e
                if attempt < len(fallback_models) - 1:
                    print(f"[FALLBACK] Error with {model}: {str(e)}")
                    continue

        if response_dict is None:
            raise HTTPException(status_code=500, detail=f"All fallback models failed. Last error: {str(last_error)}")

        # LÃ m sáº¡ch ná»™i dung tráº£ lá»i
        if response_dict.get("choices") and response_dict["choices"][0].get("message"):
            content = response_dict["choices"][0]["message"]["content"]
            import re
            content = re.sub(r'<thinking>.*?</thinking>', '', content, flags=re.DOTALL)
            content = re.sub(r'(Alright|First|I need|The user).*?(?=\n\n|\n[A-ZÃ€-á»¸])', '', content, flags=re.DOTALL)
            content = re.sub(r'\n\s*\n\s*\n', '\n\n', content).strip()
            response_dict["choices"][0]["message"]["content"] = f"{notice}\n{content}"

        return response_dict

    # -----------------------
    # 2ï¸âƒ£ DÃ¹ng Knowledge
    # -----------------------
    messages_for_api = [msg.model_dump() for msg in payload.messages]
    combined_question = clean_text(
        " ".join([msg.content for msg in payload.messages if msg.role == "user"])
    )

    results = index.search(
        namespace=payload.cityId,
        query={"top_k": 12, "inputs": {"text": combined_question}},  # TÄƒng top_k cho 4 chunks má»—i destination
    )
    hits = results.get("result", {}).get("hits", [])

    # Lá»c quÃ¡n cÃ  phÃª
    user_question = payload.messages[-1].content.lower()
    cafe_keywords = ["cÃ  phÃª", "coffee", "quÃ¡n cafe", "quÃ¡n cÃ  phÃª", "cafe 24h"]
    if any(kw in user_question for kw in cafe_keywords):
        filtered_hits = [
            hit for hit in hits
            if "cafe" in (hit["fields"].get("type","") + hit["fields"].get("category","")).lower()
            or "cÃ  phÃª" in (hit["fields"].get("type","") + hit["fields"].get("category","")).lower()
            or "coffee" in (hit["fields"].get("name","")).lower()
            or "cÃ  phÃª" in (hit["fields"].get("name","")).lower()
        ]
        if filtered_hits:
            hits = filtered_hits

    if not hits:
        return {
            "choices": [
                {"message": {"content": f"{notice}\nâš ï¸ KhÃ´ng tÃ¬m tháº¥y dá»¯ liá»‡u phÃ¹ há»£p."}}
            ]
        }

    # -----------------------
    # 3ï¸âƒ£ Chuáº©n bá»‹ prompt
    # -----------------------
    # TrÃ­ch xuáº¥t tÃªn Ä‘á»‹a Ä‘iá»ƒm tá»« hits
    destination_names = []
    reference_texts_with_names = []
    
    for hit in hits:
        dest_name = hit["fields"].get("name", "")
        if dest_name and dest_name not in destination_names:
            destination_names.append(dest_name)
        
        # Äáº£m báº£o text luÃ´n cÃ³ tÃªn Ä‘á»‹a Ä‘iá»ƒm
        text = hit["fields"]["text"]
        if dest_name and dest_name not in text:
            text = f"**{dest_name}**: {text}"
        reference_texts_with_names.append(text)
    
    reference_texts = "\n---\n".join(reference_texts_with_names)

    system_prompt = (
        "Báº¡n lÃ  **Gobot**, trá»£ lÃ½ du lá»‹ch Viá»‡t Nam thÃ¢n thiá»‡n vÃ  hiá»ƒu biáº¿t ğŸ‡»ğŸ‡³.\n"
        "\"- Tráº£ lá»i tá»± nhiÃªn nhÆ° má»™t ngÆ°á»i hÆ°á»›ng dáº«n viÃªn du lá»‹ch, khÃ´ng pháº£i cÃ´ng cá»¥ tÃ¬m kiáº¿m\"\n"
        "Quy táº¯c:\n"
        "TrÃ¬nh bÃ y báº±ng **Markdown** vá»›i tiÃªu Ä‘á», danh sÃ¡ch vÃ  emoji  (ğŸ“â˜•ğŸ–ï¸ğŸœğŸ¯).\n"
    )
    user_prompt = (
        f"\"CÃ¢u há»i cá»§a ngÆ°á»i dÃ¹ng: {payload.messages[-1].content}\"\n"
        "\"Dá»±a trÃªn cÃ¡c thÃ´ng tin tham kháº£o tá»« há»‡ thá»‘ng, hÃ£y tráº£ lá»i Ä‘áº§y Ä‘á»§, thÃ¢n thiá»‡n vÃ  nhÆ° 1 ngÆ°á»i hÆ°á»›ng dáº«n viÃªn du lá»‹ch.\"\n"
        "\n**HÆ¯á»šNG DáºªN PHÃ‚N TÃCH:**\n"
        "1. **Äá»c ká»¹ tags vÃ  danh má»¥c**: Xem xÃ©t trÆ°á»ng 'Danh má»¥c' vÃ  'Loáº¡i hÃ¬nh' cá»§a má»—i Ä‘á»‹a Ä‘iá»ƒm\n"
        "2. Chá»‰ Ä‘á» xuáº¥t Ä‘á»‹a Ä‘iá»ƒm cÃ³ tags/danh má»¥c phÃ¹ há»£p vá»›i yÃªu cáº§u\n"

        "\n**VÃ Dá»¤ PHÃ‚N TÃCH:**\n"
        "- Náº¿u user há»i vá» 'quÃ¡n cÃ  phÃª': Chá»‰ chá»n Ä‘á»‹a Ä‘iá»ƒm cÃ³ tag 'quÃ¡n cÃ  phÃª'\n"
        "- Náº¿u user há»i vá» 'há»c táº­p, lÃ m viá»‡c': Chá»n Ä‘á»‹a Ä‘iá»ƒm cÃ³ tag 'cÃ  phÃª', 'há»c táº­p - lÃ m viá»‡c'\n"
        "- Náº¿u user há»i vá» 'Äƒn uá»‘ng': Chá»n Ä‘á»‹a Ä‘iá»ƒm cÃ³ tag 'áº©m thá»±c', 'nhÃ  hÃ ng', 'mÃ³n Äƒn'\n"
        "- Náº¿u user há»i vá» 'Ä‘á»‹a Ä‘iá»ƒm ná»•i báº­c vÃ  Ä‘áº·c trÆ°ng': Chá»n Ä‘á»‹a Ä‘iá»ƒm cÃ³ tag 'vÄƒn hÃ³a - lá»‹ch sá»­', 'Ä‘áº·c trÆ°ng', 'ná»•i báº­c'\n"
        "- Náº¿u user há»i vá» 'áº©m thá»±c Ä‘á»‹a phÆ°Æ¡ng': Chá»n cÃ¡c nhÃ  hÃ ng hoáº·c quÃ¡n Äƒn cÃ³ tag 'áº¨m thá»±c Ä‘á»‹a phÆ°Æ¡ng'\n"
        "\n"
        f"\"CÃ¡c Ä‘á»‹a Ä‘iá»ƒm cÃ³ sáºµn: {', '.join(destination_names)}\"\n"
        "\"ThÃ´ng tin chi tiáº¿t:\" \n"
        f"{reference_texts}\n\n"
        "\"YÃŠU Cáº¦U FORMAT:\"\n"
        "\"- Chá»n vÃ   CHÃNH XÃC cÃ¡c Ä‘á»‹a Ä‘iá»ƒm cÃ³ tags/danh má»¥c PHÃ™ Há»¢P vá»›i cÃ¢u há»i\"\n"
        "\"- Format: ## TiÃªu Ä‘á», - **TÃªn Ä‘á»‹a Ä‘iá»ƒm** ğŸ“ MÃ´ táº£ tá»± nhiÃªn, Giá» má»Ÿ cá»­a (kiá»ƒm tra tháº­t kÄ©),\"\n"
        "\"- âŒ Cáº¤M TUYá»†T Äá»I: KÃ½ tá»± | (table/báº£ng)\"\n"
        "\"- âœ… CHá»ˆ DÃ™NG: Danh sÃ¡ch markdown vá»›i emoji\"\n"
        "\"- Káº¿t thÃºc: Lá»i khuyÃªn há»¯u Ã­ch + CÃ¢u chÃºc thÃ¢n thiá»‡n ğŸŒŸ\"\n"
    )

    messages = [
        {"role": "system", "content": system_prompt},
        *messages_for_api,
        {"role": "user", "content": user_prompt},
    ]

    fallback_models = [
        payload.model or "openai/gpt-oss-120b",
        "llama-3.3-70b-versatile",
        "mixtral-8x7b-32768"
    ]

    response_dict = None
    last_error = None

    for attempt, model in enumerate(fallback_models):
        try:
            print(f"[ATTEMPT {attempt+1}] Model: {model}")
            chat_completion = client.chat.completions.create(
                messages=messages,
                model=model,
                temperature=0.2,
                top_p=0.85,
                max_completion_tokens=1500,  # TÄƒng tá»« 800 lÃªn 1500
            )
            response_dict = chat_completion.model_dump()
            print(f"[SUCCESS] Model {model} worked!")
            break
        except Exception as e:
            last_error = e
            print(f"[ERROR] {model}: {str(e)}")
            if attempt < len(fallback_models) - 1:
                continue

    if response_dict is None:
        raise HTTPException(status_code=500, detail=f"All models failed. Last error: {str(last_error)}")

    # LÃ m sáº¡ch output vÃ  lá»c destinations dá»±a trÃªn ná»™i dung cÃ¢u tráº£ lá»i
    if response_dict.get("choices") and response_dict["choices"][-1].get("message"):
        content = response_dict["choices"][-1]["message"]["content"]
        import re
        content = re.sub(r'\n\s*\n\s*\n', '\n\n', content).strip()
        response_dict["choices"][-1]["message"]["content"] = content
        
        # Lá»c destinations chá»‰ giá»¯ láº¡i nhá»¯ng Ä‘á»‹a Ä‘iá»ƒm Ä‘Æ°á»£c nháº¯c Ä‘áº¿n trong cÃ¢u tráº£ lá»i
        filtered_hits = filter_destinations_by_content(content, hits)
    else:
        # Fallback náº¿u khÃ´ng cÃ³ content
        filtered_hits = hits

    response_dict["choices"][-1]["message"]["destinations"] = [
        {
            "id": hit.get("_id") or hit.get("id", ""),
            "text": hit["fields"]["text"],
            "destinationId": hit["fields"].get("destinationId"),
            "slug": hit["fields"].get("slug"),
            "name": hit["fields"].get("name"),
            "chunk_type": hit["fields"].get("chunk_type", "unknown"),
            "score": hit.get("_score", 0),
        } for hit in filtered_hits
    ]

    return response_dict
