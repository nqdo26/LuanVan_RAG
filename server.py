from fastapi import FastAPI, Request, HTTPException
from pydantic import BaseModel
import requests
from pinecone import Pinecone, ServerlessSpec
from groq import Groq
import re
import unicodedata
import os
from dotenv import load_dotenv
from pymongo import MongoClient
from bson import ObjectId

from models import (
    DestinationSearchPayload,
    DestinationSearchResponse,
    DestinationResult,
    ChatCompletionPayload
)

# Load environment variables
load_dotenv()
app = FastAPI()

# MongoDB init
mongo_client = MongoClient(os.getenv("MONGODB_URI", "mongodb://localhost:27017"))
db = mongo_client[os.getenv("MONGODB_DB", "travel_db")]
cities_collection = db.cities
destinations_collection = db.destinations

# Pinecone init
pc = Pinecone(
    api_key=os.getenv("PINECONE_API_KEY")
)

index_name = os.getenv("PINECONE_INDEX_NAME")

# Groq init
client = Groq(
    api_key=os.environ.get("GROQ_API_KEY"),
)

# Check if the index exists, if not create it
if not pc.has_index(index_name):
    pc.create_index_for_model(
        name=index_name,
        cloud="aws",
        region="us-east-1",
        embed={
            "model":"llama-text-embed-v2",
            "field_map":{"text": "text"}
        }
    )

# Connect to the index
index = pc.Index(index_name)


# Define endpoints
@app.get("/")
def hello_world():
    return {
        "message": "Travel Destination Search RAG System", 
        "version": "1.0.0",
        "endpoints": {
            "search": "/v1/search-destinations",
            "ingest": "/v1/ingest-destinations", 
            "chat": "/v1/chat/completions",
            "health": "/v1/keep-alive"
        }
    }

@app.head("/v1/keep-alive")
def health_check():
        return {"status": "healthy"}

@app.get("/v1/cities")
async def get_cities():
    """
    L·∫•y danh s√°ch t·∫•t c·∫£ cities ƒë·ªÉ client c√≥ th·ªÉ ch·ªçn cityId
    """
    try:
        cities = list(cities_collection.find({}, {
            "_id": 1,
            "name": 1,
            "slug": 1,
            "description": 1
        }))
        
        # Convert ObjectId to string
        for city in cities:
            city["_id"] = str(city["_id"])
        
        return {
            "cities": cities,
            "total": len(cities)
        }
        
    except Exception as e:
        print(f"Error in get_cities: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/v1/ingest-destinations")
async def ingest_destinations():
    """
    Ingest t·∫•t c·∫£ destinations t·ª´ MongoDB v√†o Pinecone vector database
    """
    try:
        # L·∫•y t·∫•t c·∫£ destinations t·ª´ MongoDB
        destinations = list(destinations_collection.find({}))
        
        records = []
        
        for dest in destinations:
            # T·∫°o text content cho embedding
            content_parts = [
                dest['title'],
                dest['details']['description'],
                " ".join(dest['details']['highlight']),
                " ".join(dest['details']['services']),
                " ".join(dest['details']['activities']),
                dest['location']['address']
            ]
            
            # L·∫•y t√™n tags
            tag_names = []
            if 'tags' in dest and dest['tags']:
                for tag_id in dest['tags']:
                    tag_doc = db.tags.find_one({"_id": tag_id})
                    if tag_doc:
                        tag_names.append(tag_doc.get('name', ''))
            
            if tag_names:
                content_parts.append(" ".join(tag_names))
            
            # T·∫°o content text
            content_text = " ".join([part for part in content_parts if part])
            
            # Clean text
            def clean_text(text: str) -> str:
                text = re.sub(r'\s+', ' ', text).strip()
                return text
            
            cleaned_content = clean_text(content_text)
            
            # T·∫°o record cho Pinecone
            record = {
                "id": f"dest-{dest['_id']}",
                "text": cleaned_content,
                "destinationId": str(dest['_id']),
                "title": dest['title'],
                "cityId": str(dest['location']['city']),
                "tags": tag_names
            }
            
            records.append(record)
        
        # Batch size c·ªßa Pinecone
        batch_size = 90
        
        # Upsert v√†o Pinecone theo batch
        for i in range(0, len(records), batch_size):
            batch = records[i:i + batch_size]
            index.upsert_records("destinations", batch)
        
        return {
            "status": "success", 
            "destinations_processed": len(records),
            "message": f"ƒê√£ ingest {len(records)} destinations v√†o Pinecone"
        }
        
    except Exception as e:
        print(f"Error in ingest_destinations: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/v1/search-destinations")
async def search_destinations(payload: DestinationSearchPayload):
    """
    T√¨m ki·∫øm ƒë·ªãa ƒëi·ªÉm du l·ªãch d·ª±a tr√™n cityId v√† purpose
    """
    try:
        # 1. T√¨m city theo ID
        city = cities_collection.find_one({"_id": ObjectId(payload.cityId)})
        if not city:
            raise HTTPException(status_code=404, detail=f"Kh√¥ng t√¨m th·∫•y th√†nh ph·ªë v·ªõi ID: {payload.cityId}")
        
        # 2. G·ªçi LLM ƒë·ªÉ ph√¢n t√≠ch purpose th√†nh tags
        tag_analysis_prompt = f"""
        ### üìç Y√™u c·∫ßu ph√¢n t√≠ch:
        Ph√¢n t√≠ch m·ª•c ƒë√≠ch du l·ªãch sau ƒë√¢y v√† tr·∫£ v·ªÅ danh s√°ch c√°c tags ph√π h·ª£p cho vi·ªác t√¨m ki·∫øm ƒë·ªãa ƒëi·ªÉm du l·ªãch.
        
        **M·ª•c ƒë√≠ch:** {payload.purpose}
        **Th√†nh ph·ªë:** {city['name']}
        
        ### üè∑Ô∏è Tags c·∫ßn tr·∫£ v·ªÅ:
        - Tr·∫£ v·ªÅ t·ªëi ƒëa 5 tags ph√π h·ª£p nh·∫•t
        - M·ªói tag ph·∫£i ng·∫Øn g·ªçn, r√µ r√†ng
        - Tags ph·∫£i li√™n quan ƒë·∫øn lo·∫°i ƒë·ªãa ƒëi·ªÉm, ho·∫°t ƒë·ªông, ho·∫∑c ƒë·∫∑c ƒëi·ªÉm du l·ªãch
        - V√≠ d·ª•: "ch·ª•p ·∫£nh" ‚Üí ["photography", "scenic", "landmark", "viewpoint", "instagram"]
        
        ### üìã Format tr·∫£ v·ªÅ:
        Ch·ªâ tr·∫£ v·ªÅ danh s√°ch tags, m·ªói tag tr√™n m·ªôt d√≤ng, kh√¥ng c√≥ ƒë√°nh s·ªë:
        tag1
        tag2
        tag3
        """
        
        tag_completion = client.chat.completions.create(
            messages=[{"role": "user", "content": tag_analysis_prompt}],
            model="deepseek-r1-distill-llama-70b",
            temperature=0.3,
            max_tokens=200
        )
        
        # Parse tags t·ª´ response
        generated_tags = []
        tag_response = tag_completion.choices[0].message.content.strip()
        for line in tag_response.split('\n'):
            tag = line.strip().lower()
            if tag and not tag.startswith('#'):
                generated_tags.append(tag)
        
        # 3. T·∫°o embedding cho c√¢u truy v·∫•n
        query_text = f"{payload.purpose} in {city['name']}"
        
        # Search trong Pinecone
        search_results = index.search(
            namespace="destinations",
            query={
                "top_k": 50,  # L·∫•y nhi·ªÅu h∆°n ƒë·ªÉ filter sau
                "inputs": {
                    'text': query_text
                }
            }
        )
        
        # 4. L·ªçc v√† x·ª≠ l√Ω k·∫øt qu·∫£
        filtered_destinations = []
        destination_ids = []
        
        for hit in search_results['result']['hits']:
            destination_id = hit['fields']['destinationId']
            destination_ids.append(destination_id)
        
        # L·∫•y th√¥ng tin chi ti·∫øt t·ª´ MongoDB
        destinations = list(destinations_collection.find({
            "_id": {"$in": [ObjectId(did) for did in destination_ids]},
            "location.city": city['_id']
        }))
        
        # T·∫°o map ƒë·ªÉ truy c·∫≠p nhanh
        destination_map = {str(dest['_id']): dest for dest in destinations}
        score_map = {hit['fields']['destinationId']: hit['_score'] for hit in search_results['result']['hits']}
        
        # L·∫•y th√¥ng tin tags cho m·ªói destination
        for dest in destinations:
            dest_id = str(dest['_id'])
            if dest_id in score_map:
                # T√≠nh ƒëi·ªÉm d·ª±a tr√™n similarity score v√† tag matching
                base_score = score_map[dest_id]
                
                # T√≠nh ƒëi·ªÉm bonus cho tag matching
                tag_bonus = 0
                if 'tags' in dest and dest['tags']:
                    # L·∫•y t√™n tags t·ª´ ObjectId
                    tag_names = []
                    for tag_id in dest['tags']:
                        tag_doc = db.tags.find_one({"_id": tag_id})
                        if tag_doc:
                            tag_names.append(tag_doc.get('name', '').lower())
                    
                    # T√≠nh s·ªë tag tr√πng kh·ªõp
                    matching_tags = set(tag_names) & set(generated_tags)
                    tag_bonus = len(matching_tags) * 0.1  # Bonus 0.1 cho m·ªói tag tr√πng
                
                final_score = base_score + tag_bonus
                
                # T·∫°o response object
                destination_result = DestinationResult(
                    title=dest['title'],
                    slug=dest['slug'],
                    tags=tag_names if 'tag_names' in locals() else [],
                    location={
                        "address": dest['location']['address'],
                        "city": city['name']
                    },
                    details={
                        "description": dest['details']['description'],
                        "highlight": dest['details']['highlight'],
                        "services": dest['details']['services'],
                        "activities": dest['details']['activities'],
                        "fee": dest['details']['fee']
                    },
                    album=dest['album'],
                    score=round(final_score, 3)
                )
                
                filtered_destinations.append(destination_result)
        
        # S·∫Øp x·∫øp theo ƒëi·ªÉm s·ªë gi·∫£m d·∫ßn
        filtered_destinations.sort(key=lambda x: x.score, reverse=True)
        
        # Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng k·∫øt qu·∫£
        limited_destinations = filtered_destinations[:payload.limit]
        
        return DestinationSearchResponse(
            city={
                "id": str(city['_id']),
                "name": city['name'],
                "slug": city['slug'],
                "description": city.get('description', '')
            },
            purpose=payload.purpose,
            generatedTags=generated_tags,
            destinations=limited_destinations,
            totalFound=len(limited_destinations)
        )
        
    except Exception as e:
        print(f"Error in search_destinations: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/v1/chat/completions")
def create_chat_completion(payload: ChatCompletionPayload):

    if not payload.isUseKnowledge:
        try:
            # Convert Pydantic Message models to dictionaries if payload.messages contains them
            messages_for_api = [message.model_dump() for message in payload.messages]

            last_message = messages_for_api[-1] if messages_for_api else None
            # Remove the last message since we'll handle it separately
            messages_for_api = messages_for_api[:-1]

            chat_completion = client.chat.completions.create(
                messages=messages_for_api + [
                     {
                        "role": "user",
                        "content": (
                            "### üìò Y√™u c·∫ßu:\n"
                            f"Tr·∫£ l·ªùi c√¢u h·ªèi sau: {last_message['content']}\n\n"
                            "### ‚úèÔ∏è Ghi ch√∫ khi tr·∫£ l·ªùi:\n"
                            "- Tr√¨nh b√†y c√¢u tr·∫£ l·ªùi b·∫±ng [Markdown] ƒë·ªÉ h·ªá th·ªëng `react-markdown` c√≥ th·ªÉ hi·ªÉn th·ªã t·ªët.\n"
                            "- Th√™m emoji ph√π h·ª£p ƒë·ªÉ l√†m n·ªïi b·∫≠t n·ªôi dung ch√≠nh üß†üìåüí°.\n" 
                            "- N·∫øu n·ªôi dung c√≥ th·ªÉ so s√°nh ho·∫∑c ph√¢n lo·∫°i, h√£y s·ª≠ d·ª•ng **b·∫£ng Markdown** ƒë·ªÉ tr√¨nh b√†y.\n"
                        )
                     }
                ],
                model=payload.model or "deepseek-r1-distill-llama-70b",  # Use model from payload or default
                # You can pass other parameters from payload to the API call if needed
                # e.g., temperature=payload.temperature
                temperature=0.5,
                max_completion_tokens=1024,
                top_p=1,
            )

            return chat_completion
        
        except Exception as e:
            print(f"Error during chat completion: {e}") # For server-side logging
            raise HTTPException(status_code=500, detail=str(e))
        
    else:
         
        messages_for_api = [message.model_dump() for message in payload.messages]

        # Clean the question for the query
        def clean_text(text: str) -> str:
            # 1. Chuy·ªÉn v·ªÅ ch·ªØ th∆∞·ªùng
            # text = text.lower()

            # 2. Chu·∫©n h√≥a Unicode (d√πng NFC ƒë·ªÉ gh√©p d·∫•u)
            text = unicodedata.normalize("NFC", text)

            # 3. Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát (gi·ªØ l·∫°i ti·∫øng Vi·ªát v√† ch·ªØ s·ªë)
            text = re.sub(r"[^\w\s√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©"
                        r"√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë]", "", text)

            # 4. Lo·∫°i b·ªè kho·∫£ng tr·∫Øng d∆∞ th·ª´a
            text = re.sub(r"\s+", " ", text).strip()

            return text


        # Combine all previous user question into a single string for the query
        combined_question = [message.model_dump() for message in payload.messages]
        combined_question = [message for message in combined_question if message['role'] == 'user']
        combined_question = [message['content'] for message in combined_question]
        combined_question = " ".join(combined_question)
        combined_question = clean_text(combined_question)
        print(combined_question)




        # Search the dense index
        query = {
            "top_k": 15,
            "inputs": {
                # 'text': clean_text(payload.messages[len(payload.messages) - 1].content)
                'text': combined_question
            }
        }
        if payload.courseId:
            query["filter"] = {"courseId": payload.courseId}

        results = index.search(
            namespace=payload.userId,
            query=query
        )
        # results = index.search(
        #     namespace=payload.userId,
        #     query={
        #         "top_k": 15,
        #         "inputs": {
        #             'text': clean_text(payload.messages[len(payload.messages) - 1].content)
        #         },
        #         "filter": {
        #             "courseId": payload.courseId
        #         } if payload.courseId else None
        #     },
        # )

        # Print the results
        # for hit in results['result']['hits']:
        #         print(f"id: {hit['_id']:<5} | documentId: {hit['fields']['documentId']} | title: {hit['fields']['title']} | score: {round(hit['_score'], 2):<5} | text: {hit['fields']['text']:<50}")
                

        chat_completion = client.chat.completions.create(
            messages=messages_for_api + [
                {
                    "role": "user",
                    "content": (
                        "### üìò Y√™u c·∫ßu:\n"
                        f"Tr·∫£ l·ªùi c√¢u h·ªèi sau b·∫±ng c√°ch d·ª±a tr√™n c√°c ƒëo·∫°n vƒÉn b√™n d∆∞·ªõi. "
                        "N·∫øu th√¥ng tin kh√¥ng ƒë·ªß, h√£y tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c c·ªßa b·∫°n v√† ghi r√µ ƒëi·ªÅu ƒë√≥.\n\n"
                        f"**C√¢u h·ªèi:** {payload.messages[len(payload.messages) - 1].content}\n\n"
                        "### üìö ƒêo·∫°n vƒÉn tham kh·∫£o:\n"
                        + "\n---\n".join([
                            f"**ƒêo·∫°n vƒÉn {i+1} (Document title: {hit['fields']['title']}):**\n"
                            f"{hit['fields']['text']}\n"
                            for i, hit in enumerate(results['result']['hits'])
                        ]) +
                        "### ‚úèÔ∏è Ghi ch√∫ khi tr·∫£ l·ªùi:\n"
                        "- Tr√¨nh b√†y c√¢u tr·∫£ l·ªùi b·∫±ng [Markdown] ƒë·ªÉ h·ªá th·ªëng `react-markdown` c√≥ th·ªÉ hi·ªÉn th·ªã t·ªët.\n"
                        "- ƒê·∫£m b·∫£o m·ªói th√¥ng tin ƒë∆∞·ª£c tr√≠ch d·∫´n ƒë·ªÅu c√≥ tham chi·∫øu ƒë·∫øn **Document title** t∆∞∆°ng ·ª©ng (v√≠ d·ª•: `[Python ƒë·∫°i c∆∞∆°ng]` ch·ªâ c·∫ßn t·ª±a c·ªßa t√†i li·ªáu g·ªëc, kh√¥ng c·∫ßn ghi ƒëo·∫°n vƒÉn n√†o, kh√¥ng nh·∫Øc l·∫°i 'Document title' v√† kh√¥ng nh·∫Øc l·∫°i t·ª±a t√†i li·ªáu n·∫øu b·ªã l·∫∑p).\n"
                        "- Th√™m emoji ph√π h·ª£p ƒë·ªÉ l√†m n·ªïi b·∫≠t n·ªôi dung ch√≠nh üß†üìåüí°.\n"
                        "- N·∫øu n·ªôi dung c√≥ th·ªÉ so s√°nh ho·∫∑c ph√¢n lo·∫°i, h√£y s·ª≠ d·ª•ng **b·∫£ng Markdown** ƒë·ªÉ tr√¨nh b√†y.\n"
                        "- N·∫øu c√¢u tr·∫£ l·ªùi kh√¥ng th·ªÉ r√∫t ra t·ª´ ƒëo·∫°n vƒÉn, h√£y b·∫Øt ƒë·∫ßu b·∫±ng c√¢u: `‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y th√¥ng tin trong ƒëo·∫°n vƒÉn, c√¢u tr·∫£ l·ªùi ƒë∆∞·ª£c t·∫°o t·ª´ ki·∫øn th·ª©c n·ªÅn.`\n"  
                    )
                }
            ],
            model=payload.model or "deepseek-r1-distill-llama-70b",
        )

        response_dict = chat_completion.model_dump()

        response_dict["choices"][len(response_dict["choices"])-1]["message"]["documents"] = [
            {
                "id": hit["_id"],
                "text": hit["fields"]["text"],
                "documentId": hit["fields"]["documentId"],
                "score": hit["_score"]
            } for hit in results['result']['hits']
        ]
        return response_dict
    
