from fastapi import FastAPI, Request, HTTPException
from fastapi.exceptions import RequestValidationError
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import requests
from pinecone import Pinecone, ServerlessSpec
from groq import Groq
from langchain.text_splitter import RecursiveCharacterTextSplitter
import re
import unicodedata
import os
from dotenv import load_dotenv

from models import IngestPayload, QuestionPayload, DeletePayload, ChatCompletionPayload, UpdatePayload

# Load environment variables
load_dotenv()
app = FastAPI()

# Th√™m exception handler cho validation errors
@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    print(f"[VALIDATION ERROR] {exc.errors()}")
    print(f"[REQUEST BODY] {await request.body()}")
    return JSONResponse(
        status_code=422,
        content={"detail": exc.errors(), "body": str(await request.body())}
    )

# Pinecone init
pc = Pinecone(
    api_key=os.getenv("PINECONE_API_KEY")
)

index_name = os.getenv("PINECONE_INDEX_NAME")

# Groq init
client = Groq(
    api_key=os.environ.get("GROQ_API_KEY"),
)

# Define the text splitter v·ªõi semantic separators cho du l·ªãch
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,  # TƒÉng size ƒë·ªÉ gi·ªØ nguy√™n th√¥ng tin
    chunk_overlap=100,  # TƒÉng overlap ƒë·ªÉ gi·ªØ context
    separators=[
        "\n### ",  # Ph√¢n chia theo section headers
        "\n## ",   # Headers nh·ªè h∆°n
        "\n- ",    # List items
        ".\n",     # K·∫øt th√∫c c√¢u + newline
        ". ",      # K·∫øt th√∫c c√¢u
        "\n",      # Newline
        " "        # Space (fallback)
    ]
)

# Check if the index exists, if not create it
if not pc.has_index(index_name):
    pc.create_index_for_model(
        name=index_name,
        cloud="aws",
        region="us-east-1",
        embed={
            "model":"llama-text-embed-v2",
            "field_map":{"text": "text"}
        }
    )

# Connect to the index
index = pc.Index(index_name)


# Define endpoints
@app.get("/")
def hello_world():
    return {"message": "Hello World!"}

@app.head("/v1/keep-alive")
def health_check():
        return {"status": "healthy"}

@app.post("/v1/ingest")
def ingest(payload: IngestPayload):
        
        # Parse JSON data t·ª´ backend
        import json
        try:
            destination_data = json.loads(payload.info)
        except:
            # Fallback n·∫øu v·∫´n l√† string c≈©
            destination_data = {"description": payload.info}
        
        # Semantic chunking theo lo·∫°i destination
        chunks = create_semantic_chunks(payload.name, destination_data, payload.destinationId, payload.slug)

        # T·∫°o records v·ªõi metadata ƒë·∫ßy ƒë·ªß
        records = [
            {
                "id": f"{payload.destinationId}-{chunk['type']}",
                "text": chunk['content'],
                'destinationId': payload.destinationId,
                'slug': payload.slug,
                'name': payload.name,
                'chunk_type': chunk['type'],
                'chunk_index': i,
                'total_chunks': len(chunks)
            } for i, chunk in enumerate(chunks)
        ]

        # Batch size of 90 (below Pinecone's limit of 96)
        batch_size = 90
    
        # Split records into batches and upsert
        for i in range(0, len(records), batch_size):
            batch = records[i:i + batch_size]
            index.upsert_records(payload.cityId, batch)

        return {"status": "done", "chunks_processed": len(records)}

@app.post("/v1/update")
def update_destination(payload: UpdatePayload):
    """
    C·∫≠p nh·∫≠t destination tr√™n Pinecone - x√≥a chunks c≈© v√† t·∫°o chunks m·ªõi
    X·ª≠ l√Ω c·∫£ tr∆∞·ªùng h·ª£p thay ƒë·ªïi namespace (cityId)
    """
    try:

        
        # B∆∞·ªõc 1: T√¨m v√† x√≥a c√°c chunks c≈© t·ª´ T·∫§T C·∫¢ namespaces
        # V√¨ c√≥ th·ªÉ cityId ƒë√£ thay ƒë·ªïi, ta c·∫ßn t√¨m trong t·∫•t c·∫£ namespaces
        deleted_count = 0
        
        # L·∫•y danh s√°ch t·∫•t c·∫£ namespaces
        try:
            stats = index.describe_index_stats()
            all_namespaces = list(stats.get('namespaces', {}).keys())
            print(f"[UPDATE] Searching in namespaces: {all_namespaces}")
            
            # T√¨m v√† x√≥a chunks c≈© trong t·∫•t c·∫£ namespaces
            for namespace in all_namespaces:
                try:
                    ids_to_delete = list(index.list(prefix=payload.destinationId, namespace=namespace))
                    if ids_to_delete:
                        index.delete(namespace=namespace, ids=ids_to_delete)
                        deleted_count += len(ids_to_delete)
                        print(f"[UPDATE] Deleted {len(ids_to_delete)} chunks from namespace {namespace}")
                except Exception as ns_error:
                    print(f"[UPDATE] Error checking namespace {namespace}: {ns_error}")
                    continue
                    
        except Exception as stats_error:
            print(f"[UPDATE] Could not get index stats, trying current namespace only: {stats_error}")
            # Fallback: ch·ªâ x√≥a t·ª´ namespace hi·ªán t·∫°i
            try:
                ids_to_delete = list(index.list(prefix=payload.destinationId, namespace=payload.cityId))
                if ids_to_delete:
                    index.delete(namespace=payload.cityId, ids=ids_to_delete)
                    deleted_count = len(ids_to_delete)
                    print(f"[UPDATE] Deleted {deleted_count} chunks from current namespace {payload.cityId}")
            except Exception as fallback_error:
                print(f"[UPDATE] Fallback delete also failed: {fallback_error}")
        
        # B∆∞·ªõc 2: Parse JSON data t·ª´ backend (gi·ªëng nh∆∞ ingest)
        import json
        try:
            destination_data = json.loads(payload.info)
            print(f"[UPDATE] Parsed destination data keys: {list(destination_data.keys())}")
        except Exception as parse_error:
            print(f"[UPDATE ERROR] JSON parse failed: {parse_error}")
            # Fallback n·∫øu v·∫´n l√† string c≈©
            destination_data = {"description": payload.info}
        
        # B∆∞·ªõc 3: T·∫°o semantic chunks m·ªõi v·ªõi 4 chunks
        chunks = create_semantic_chunks(payload.name, destination_data, payload.destinationId, payload.slug)
        print(f"[UPDATE] Created {len(chunks)} chunks")

        # B∆∞·ªõc 4: T·∫°o records m·ªõi v·ªõi metadata ƒë·∫ßy ƒë·ªß
        records = [
            {
                "id": f"{payload.destinationId}-{chunk['type']}",
                "text": chunk['content'],
                'destinationId': payload.destinationId,
                'slug': payload.slug,
                'name': payload.name,
                'chunk_type': chunk['type'],
                'chunk_index': i,
                'total_chunks': len(chunks)
            } for i, chunk in enumerate(chunks)
        ]

        # B∆∞·ªõc 5: Upsert records m·ªõi v√†o namespace m·ªõi (cityId t·ª´ payload)
        batch_size = 90
        
        for i in range(0, len(records), batch_size):
            batch = records[i:i + batch_size]
            index.upsert_records(payload.cityId, batch)

        print(f"[UPDATE] Created {len(records)} new chunks for destination {payload.destinationId} in namespace {payload.cityId}")
        
        return {
            "status": "updated", 
            "chunks_deleted": deleted_count,
            "chunks_created": len(records),
            "new_namespace": payload.cityId
        }
        
    except Exception as e:
        print(f"[UPDATE ERROR] {str(e)}")
        raise HTTPException(status_code=500, detail=f"Update failed: {str(e)}")

@app.post("/v1/question")
def question(payload: QuestionPayload):
    # Define the query

    # Search the dense index v·ªõi tƒÉng top_k ƒë·ªÉ bao ph·ªß t·ªët h∆°n
    results = index.search(
        namespace=payload.cityId,
        query={
            "top_k": 12,  # TƒÉng l√™n v√¨ m·ªói destination c√≥ 4 chunks
            "inputs": {
                'text': payload.query
            }
        }
    )

    # Print the results v·ªõi th√¥ng tin chunk type
    for hit in results['result']['hits']:
            chunk_type = hit['fields'].get('chunk_type', 'unknown')
            print(f"id: {hit['_id']:<5} | type: {chunk_type:<10} | destinationId: {hit['fields']['destinationId']} | text: {hit['fields']['text'][:50]}")
            

    chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": (
                "### üìò Y√™u c·∫ßu:\n"
                f"Tr·∫£ l·ªùi c√¢u h·ªèi sau b·∫±ng c√°ch d·ª±a tr√™n c√°c ƒëo·∫°n vƒÉn b√™n d∆∞·ªõi. "
                "N·∫øu th√¥ng tin kh√¥ng ƒë·ªß, h√£y tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c c·ªßa b·∫°n v√† ghi r√µ ƒëi·ªÅu ƒë√≥.\n\n"
                f"**C√¢u h·ªèi:** {payload.query}\n\n"
                "### üìö ƒêo·∫°n vƒÉn tham kh·∫£o:\n"
                # + "\n---\n".join([hit['fields']['text'] for hit in results['result']['hits']]) +
                # "\n\n"
                + "\n---\n".join([
                     f"**ƒêo·∫°n vƒÉn {i+1}:**\n"
                     f"{hit['fields']['text']}\n"
                     for i, hit in enumerate(results['result']['hits'])
                     ]) +
                "### ‚úèÔ∏è Ghi ch√∫ khi tr·∫£ l·ªùi:\n"
                "- Tr√¨nh b√†y c√¢u tr·∫£ l·ªùi b·∫±ng [Markdown] ƒë·ªÉ h·ªá th·ªëng `react-markdown` c√≥ th·ªÉ hi·ªÉn th·ªã t·ªët.\n"
                "- Th√™m emoji ph√π h·ª£p ƒë·ªÉ l√†m n·ªïi b·∫≠t n·ªôi dung ch√≠nh üß†üìåüí°.\n"
                "- N·∫øu c√¢u tr·∫£ l·ªùi kh√¥ng th·ªÉ r√∫t ra t·ª´ ƒëo·∫°n vƒÉn, h√£y b·∫Øt ƒë·∫ßu b·∫±ng c√¢u: `D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë g·ª£i √Ω c·ªßa t√¥i, ƒë·ªÉ c√≥ th·ªÉ nh·∫≠n g·ª£i √Ω ch√≠nh x√°c h∆°n t·ª´ h·ªá th·ªëng, vui l√≤ng ch·ªçn ƒëi·ªÉm ƒë·∫øn tr∆∞·ªõc nh√©`"
            )
        }
    ],
    model=payload.model or "deepseek-r1-distill-llama-70b",
    )
    response_dict = chat_completion.model_dump()
    response_dict["choices"][0]["message"]["documents"] = [
        {
            "id": hit["_id"],
            "text": hit["fields"]["text"],
            "destinationId": hit["fields"]["destinationId"],
            "score": hit["_score"]
        } for hit in results['result']['hits']
    ]
    return response_dict

@app.post("/v1/delete-document")
def delete_document(payload: DeletePayload):

    ids_to_delete = list(index.list(prefix=payload.destiationId, namespace=payload.cityId))

    if not ids_to_delete:
        raise HTTPException(status_code=404, detail="Kh√¥ng t√¨m th·∫•y vectors n√†o v·ªõi documentId n√†y.")

    # B∆∞·ªõc 2: Xo√° vector theo ID
    index.delete(
        namespace=payload.cityId,
        ids=ids_to_delete
    )

    return {"deleted_ids": ids_to_delete}

# H√†m clean_text ƒë·ªÉ x·ª≠ l√Ω vƒÉn b·∫£n
def clean_text(text: str) -> str:
    """
    L√†m s·∫°ch v√† chu·∫©n h√≥a vƒÉn b·∫£n ti·∫øng Vi·ªát
    """
    # 1. Chu·∫©n h√≥a Unicode (d√πng NFC ƒë·ªÉ gh√©p d·∫•u)
    text = unicodedata.normalize("NFC", text)

    # 2. Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát (gi·ªØ l·∫°i ti·∫øng Vi·ªát v√† ch·ªØ s·ªë)
    text = re.sub(r"[^\w\s√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©"
                r"√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë]", "", text)

    # 3. Lo·∫°i b·ªè kho·∫£ng tr·∫Øng d∆∞ th·ª´a
    text = re.sub(r"\s+", " ", text).strip()

    return text

def create_semantic_chunks(name: str, data: dict, destination_id: str, slug: str = None) -> list:
    """
    T·∫°o 4 chunks semantic cho m·ªói destination theo strategy m·ªõi
    """
    chunks = []
    
    # 1. T·ªïng quan - Th√¥ng tin t·ªïng quan (kh√¥ng l·∫∑p t√™n)
    overview_content = ""
    
    # M√¥ t·∫£ ch√≠nh
    if data.get('description'):
        overview_content += f"{data['description']}\n\n"
    
    # ƒêi·ªÉm n·ªïi b·∫≠t
    if data.get('highlight'):
        overview_content += f"**ƒêi·ªÉm n·ªïi b·∫≠t:** {data['highlight']}\n\n"
    
    # Lo·∫°i h√¨nh du l·ªãch
    if data.get('cultureType'):
        overview_content += f"**Lo·∫°i h√¨nh:** {data['cultureType']}\n\n"
    
    chunks.append({
        'type': 'tong-quan',
        'content': overview_content.strip()
    })
    
    # 2. Tr·∫£i nghi·ªám - Tr·∫£i nghi·ªám v√† ho·∫°t ƒë·ªông (kh√¥ng l·∫∑p t√™n)
    experience_content = ""
    
    # D·ªãch v·ª•
    if data.get('services'):
        experience_content += f"**D·ªãch v·ª•:** {data['services']}\n\n"
    
    # Ho·∫°t ƒë·ªông
    if data.get('activities'):
        experience_content += f"**Ho·∫°t ƒë·ªông:** {data['activities']}\n\n"
    
    # Th√¥ng tin h·ªØu √≠ch
    if data.get('usefulInfo'):
        experience_content += f"**Th√¥ng tin h·ªØu √≠ch:** {data['usefulInfo']}\n\n"
    
    chunks.append({
        'type': 'trai-nghiem',
        'content': experience_content.strip()
    })
    
    # 3. Th·ª±c t·∫ø - Th√¥ng tin th·ª±c t·∫ø (kh√¥ng l·∫∑p t√™n)
    practical_content = ""
    
    # Gi·ªù m·ªü c·ª≠a
    if data.get('openHour'):
        practical_content += f"**Gi·ªù m·ªü c·ª≠a:** {data['openHour']}\n\n"
    
    # Ph√≠ tham quan
    if data.get('fee'):
        practical_content += f"**Ph√≠ tham quan:** {data['fee']}\n\n"
    
    # Th√¥ng tin li√™n h·ªá
    if data.get('contactInfo'):
        practical_content += f"**Li√™n h·ªá:** {data['contactInfo']}\n\n"
    
    chunks.append({
        'type': 'thuc-te',
        'content': practical_content.strip()
    })
    
    # 4. Danh m·ª•c - Tags v√† t·ª´ kh√≥a t√¨m ki·∫øm
    tags_content = ""
    
    # Tags ch√≠nh
    if data.get('tags'):
        tags_content += f"**Danh m·ª•c:** {data['tags']}\n\n"
    
    # B·ªï sung th√¥ng tin v·ªÅ lo·∫°i h√¨nh ƒë·ªÉ tƒÉng kh·∫£ nƒÉng t√¨m ki·∫øm
    if data.get('cultureType'):
        tags_content += f"**VƒÉn h√≥a:** {data['cultureType']}\n\n"
    
    # T√™n ƒë·ªãa ƒëi·ªÉm ƒë·ªÉ tƒÉng ƒë·ªô ch√≠nh x√°c t√¨m ki·∫øm
    tags_content += f"**T√™n ƒë·ªãa ƒëi·ªÉm:** {name}\n\n"
    
    chunks.append({
        'type': 'danh-muc',
        'content': tags_content.strip()
    })
    
    return chunks

@app.post("/v1/chat/completions")
def create_chat_completion(payload: ChatCompletionPayload):
    """
    Chat completion cho Gobot - tr·ª£ l√Ω du l·ªãch Vi·ªát Nam
    """
    # Th√¥ng b√°o chung khi ch∆∞a ch·ªçn th√†nh ph·ªë
    notice = (
        "üëã Xin ch√†o! Hi·ªán t·∫°i b·∫°n **ch∆∞a ch·ªçn th√†nh ph·ªë** ho·∫∑c ƒëi·ªÉm ƒë·∫øn c·ª• th·ªÉ.\n"
        "ƒê·ªÉ nh·∫≠n g·ª£i √Ω **ch√≠nh x√°c t·ª´ h·ªá th·ªëng**, h√£y ch·ªçn m·ªôt th√†nh ph·ªë tr∆∞·ªõc nh√©! üèôÔ∏è\n"
    )

    # -----------------------
    # 1Ô∏è‚É£ Kh√¥ng d√πng Knowledge
    # -----------------------
    if not payload.isUseKnowledge or not payload.cityId:
        messages_for_api = [msg.model_dump() for msg in payload.messages]
        user_question = messages_for_api[-1]["content"] if messages_for_api else ""

        system_prompt = (
            "B·∫°n l√† **Gobot**, tr·ª£ l√Ω du l·ªãch th√¥ng minh v√† th√¢n thi·ªán c·ªßa website **GoOhNo**, n·ªÅn t·∫£ng h·ªó tr·ª£ l√™n k·∫ø ho·∫°ch du l·ªãch Vi·ªát Nam.\n"
            "B·∫°n ƒë√≥ng vai tr√≤ nh∆∞ m·ªôt **h∆∞·ªõng d·∫´n vi√™n b·∫£n ƒë·ªãa**, tr√≤ chuy·ªán t·ª± nhi√™n v√† g·∫ßn g≈©i ƒë·ªÉ gi√∫p ng∆∞·ªùi d√πng kh√°m ph√° Vi·ªát Nam d·ªÖ d√†ng.\n"
            "Quy t·∫Øc quan tr·ªçng:\n"
            "1. Ch·ªâ t∆∞ v·∫•n v·ªÅ c√°c ƒë·ªãa ƒëi·ªÉm, ho·∫°t ƒë·ªông v√† tr·∫£i nghi·ªám du l·ªãch t·∫°i Vi·ªát Nam.\n"
            "2. Tr·∫£ l·ªùi b·∫±ng **ti·∫øng Vi·ªát**, gi·ªçng ƒëi·ªáu th√¢n thi·ªán, g·∫ßn g≈©i, d·ªÖ hi·ªÉu.\n"
            "3. Tr√¨nh b√†y b·∫±ng **Markdown** v·ªõi ti√™u ƒë·ªÅ, danh s√°ch v√† emoji minh h·ªça (üìçüèñÔ∏è‚òïüçúüèØ).\n"
            "4. N·∫øu kh√¥ng ch·∫Øc ch·∫Øn, h√£y n√≥i: *T√¥i kh√¥ng ch·∫Øc v·ªÅ ƒëi·ªÅu n√†y.*\n"
            "5. Cu·ªëi c√¢u tr·∫£ l·ªùi, th√™m **l·ªùi khuy√™n h·ªØu √≠ch cho du kh√°ch** v√† nh·∫Øc nh·∫π r·∫±ng h·ªç c√≥ th·ªÉ t√¨m hi·ªÉu th√™m tr√™n GoOhNo.\n"
        )


        user_prompt = (
            f"\"C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: {user_question}\"\n"
            "\"H√£y tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c n·ªÅn v·ªÅ du l·ªãch Vi·ªát Nam.\"\n"
            "\"ƒê∆∞a ra c√°c g·ª£i √Ω chi ti·∫øt, d·ªÖ ƒë·ªçc, k√®m emoji minh h·ªça.\"\n"
            "\"Chia nh·ªè n·ªôi dung th√†nh m·ª•c ho·∫∑c danh s√°ch Markdown.\"\n"
            "\"K·∫øt th√∫c b·∫±ng l·ªùi khuy√™n h·ªØu √≠ch v√† th√¢n thi·ªán.\"\n"
        )

        messages = [
            {"role": "system", "content": system_prompt},
            *messages_for_api,
            {"role": "user", "content": user_prompt},
        ]

        fallback_models = [
            payload.model or "llama-3.3-70b-versatile",
            "llama-3.1-70b-versatile",
            "mixtral-8x7b-32768"
        ]

        response_dict = None
        last_error = None

        for attempt, model in enumerate(fallback_models):
            try:
                print(f"[NO-KNOWLEDGE ATTEMPT {attempt+1}] Model: {model}")
                chat_completion = client.chat.completions.create(
                    messages=messages,
                    model=model,
                    temperature=0.4,
                    top_p=0.9,
                    max_completion_tokens=1024,
                )
                response_dict = chat_completion.model_dump()
                print(f"[SUCCESS] Model {model} worked!")
                break
            except Exception as e:
                last_error = e
                if attempt < len(fallback_models) - 1:
                    print(f"[FALLBACK] Error with {model}: {str(e)}")
                    continue

        if response_dict is None:
            raise HTTPException(status_code=500, detail=f"All fallback models failed. Last error: {str(last_error)}")

        # L√†m s·∫°ch n·ªôi dung tr·∫£ l·ªùi
        if response_dict.get("choices") and response_dict["choices"][0].get("message"):
            content = response_dict["choices"][0]["message"]["content"]
            import re
            content = re.sub(r'<thinking>.*?</thinking>', '', content, flags=re.DOTALL)
            content = re.sub(r'(Alright|First|I need|The user).*?(?=\n\n|\n[A-Z√Ä-·ª∏])', '', content, flags=re.DOTALL)
            content = re.sub(r'\n\s*\n\s*\n', '\n\n', content).strip()
            response_dict["choices"][0]["message"]["content"] = f"{notice}\n{content}"

        return response_dict

    # -----------------------
    # 2Ô∏è‚É£ D√πng Knowledge
    # -----------------------
    messages_for_api = [msg.model_dump() for msg in payload.messages]
    combined_question = clean_text(
        " ".join([msg.content for msg in payload.messages if msg.role == "user"])
    )

    results = index.search(
        namespace=payload.cityId,
        query={"top_k": 12, "inputs": {"text": combined_question}},  # TƒÉng top_k cho 4 chunks m·ªói destination
    )
    hits = results.get("result", {}).get("hits", [])

    # L·ªçc qu√°n c√† ph√™
    user_question = payload.messages[-1].content.lower()
    cafe_keywords = ["c√† ph√™", "coffee", "qu√°n cafe", "qu√°n c√† ph√™", "cafe 24h"]
    if any(kw in user_question for kw in cafe_keywords):
        filtered_hits = [
            hit for hit in hits
            if "cafe" in (hit["fields"].get("type","") + hit["fields"].get("category","")).lower()
            or "c√† ph√™" in (hit["fields"].get("type","") + hit["fields"].get("category","")).lower()
            or "coffee" in (hit["fields"].get("name","")).lower()
            or "c√† ph√™" in (hit["fields"].get("name","")).lower()
        ]
        if filtered_hits:
            hits = filtered_hits

    if not hits:
        return {
            "choices": [
                {"message": {"content": f"{notice}\n‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ph√π h·ª£p."}}
            ]
        }

    # -----------------------
    # 3Ô∏è‚É£ Chu·∫©n b·ªã prompt
    # -----------------------
    # Tr√≠ch xu·∫•t t√™n ƒë·ªãa ƒëi·ªÉm t·ª´ hits
    destination_names = []
    reference_texts_with_names = []
    
    for hit in hits:
        dest_name = hit["fields"].get("name", "")
        if dest_name and dest_name not in destination_names:
            destination_names.append(dest_name)
        
        # ƒê·∫£m b·∫£o text lu√¥n c√≥ t√™n ƒë·ªãa ƒëi·ªÉm
        text = hit["fields"]["text"]
        if dest_name and dest_name not in text:
            text = f"**{dest_name}**: {text}"
        reference_texts_with_names.append(text)
    
    reference_texts = "\n---\n".join(reference_texts_with_names)

    system_prompt = (
        "B·∫°n l√† **Gobot**, tr·ª£ l√Ω du l·ªãch Vi·ªát Nam th√¢n thi·ªán v√† hi·ªÉu bi·∫øt üáªüá≥.\n"
        "Quy t·∫Øc:\n"
        "1. Ch·ªâ t∆∞ v·∫•n c√°c ƒë·ªãa ƒëi·ªÉm v√† tr·∫£i nghi·ªám t·∫°i Vi·ªát Nam.\n"
        "2. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, gi·ªçng ƒëi·ªáu t·ª± nhi√™n, d·ªÖ g·∫ßn, nh∆∞ ƒëang tr√≤ chuy·ªán.\n"
        "3. Tr√¨nh b√†y b·∫±ng **Markdown** v·ªõi ti√™u ƒë·ªÅ, danh s√°ch v√† emoji (üìç‚òïüèñÔ∏èüçúüèØ).\n"
        "4. **QUAN TR·ªåNG**: Lu√¥n s·ª≠ d·ª•ng T√äN CH√çNH X√ÅC c·ªßa ƒë·ªãa ƒëi·ªÉm t·ª´ d·ªØ li·ªáu ƒë∆∞·ª£c cung c·∫•p.\n"
        "5. N·∫øu kh√¥ng ch·∫Øc ch·∫Øn, h√£y n√≥i: *T√¥i kh√¥ng ch·∫Øc v·ªÅ ƒëi·ªÅu n√†y.*\n"
        "6. K·∫øt th√∫c c√¢u tr·∫£ l·ªùi b·∫±ng **l·ªùi khuy√™n h·ªØu √≠ch cho kh√°ch du l·ªãch**.\n"
    )

    user_prompt = (
        f"\"C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: {payload.messages[-1].content}\"\n"
        "\"D·ª±a tr√™n c√°c th√¥ng tin tham kh·∫£o t·ª´ h·ªá th·ªëng, h√£y tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß v√† th√¢n thi·ªán.\"\n"
        f"\"C√°c ƒë·ªãa ƒëi·ªÉm c√≥ s·∫µn: {', '.join(destination_names)}\"\n"
        "\"Th√¥ng tin chi ti·∫øt:\" \n"
        f"{reference_texts}\n\n"
        "\"QUAN TR·ªåNG: H√£y s·ª≠ d·ª•ng CH√çNH X√ÅC t√™n ƒë·ªãa ƒëi·ªÉm t·ª´ danh s√°ch tr√™n.\"\n"
        "\"Tr√¨nh b√†y d∆∞·ªõi d·∫°ng danh s√°ch Markdown v·ªõi emoji, c√≥ t√™n ƒë·ªãa ƒëi·ªÉm r√µ r√†ng.\"\n"
        "\"K·∫øt th√∫c b·∫±ng m·ªôt l·ªùi khuy√™n h·ªØu √≠ch v√† th√¢n thi·ªán.\"\n"
    )

    messages = [
        {"role": "system", "content": system_prompt},
        *messages_for_api,
        {"role": "user", "content": user_prompt},
    ]

    fallback_models = [
        payload.model or "deepseek-r1-distill-llama-70b",
        "llama-3.3-70b-versatile",
        "llama-3.1-70b-versatile",
        "mixtral-8x7b-32768"
    ]

    response_dict = None
    last_error = None

    for attempt, model in enumerate(fallback_models):
        try:
            print(f"[ATTEMPT {attempt+1}] Model: {model}")
            chat_completion = client.chat.completions.create(
                messages=messages,
                model=model,
                temperature=0.2,
                top_p=0.85,
                max_completion_tokens=800,
            )
            response_dict = chat_completion.model_dump()
            print(f"[SUCCESS] Model {model} worked!")
            break
        except Exception as e:
            last_error = e
            print(f"[ERROR] {model}: {str(e)}")
            if attempt < len(fallback_models) - 1:
                continue

    if response_dict is None:
        raise HTTPException(status_code=500, detail=f"All models failed. Last error: {str(last_error)}")

    # L√†m s·∫°ch output v√† tr·∫£ v·ªÅ k√®m danh s√°ch ƒëi·ªÉm ƒë·∫øn
    if response_dict.get("choices") and response_dict["choices"][-1].get("message"):
        content = response_dict["choices"][-1]["message"]["content"]
        import re
        content = re.sub(r'\n\s*\n\s*\n', '\n\n', content).strip()
        response_dict["choices"][-1]["message"]["content"] = content

    response_dict["choices"][-1]["message"]["destinations"] = [
        {
            "id": hit.get("_id") or hit.get("id", ""),
            "text": hit["fields"]["text"],
            "destinationId": hit["fields"].get("destinationId"),
            "slug": hit["fields"].get("slug"),
            "name": hit["fields"].get("name"),
            "chunk_type": hit["fields"].get("chunk_type", "unknown"),
            "score": hit.get("_score", 0),
        } for hit in hits
    ]

    return response_dict
