from fastapi import FastAPI, Request, HTTPException
from fastapi.exceptions import RequestValidationError
from fastapi.responses import JSONResponse
from pydantic import BaseModel
import requests
from pinecone import Pinecone, ServerlessSpec
from groq import Groq
from langchain.text_splitter import RecursiveCharacterTextSplitter
import re
import unicodedata
import os
from dotenv import load_dotenv

from models import IngestPayload, QuestionPayload, DeletePayload, ChatCompletionPayload, UpdatePayload

# Load environment variables
load_dotenv()
app = FastAPI()

# Th√™m exception handler cho validation errors
@app.exception_handler(RequestValidationError)
async def validation_exception_handler(request: Request, exc: RequestValidationError):
    print(f"[VALIDATION ERROR] {exc.errors()}")
    print(f"[REQUEST BODY] {await request.body()}")
    return JSONResponse(
        status_code=422,
        content={"detail": exc.errors(), "body": str(await request.body())}
    )

# Pinecone init
pc = Pinecone(
    api_key=os.getenv("PINECONE_API_KEY")
)

index_name = os.getenv("PINECONE_INDEX_NAME")

# Groq init
client = Groq(
    api_key=os.environ.get("GROQ_API_KEY"),
)

# Define the text splitter v·ªõi semantic separators cho du l·ªãch
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=800,  # TƒÉng size ƒë·ªÉ gi·ªØ nguy√™n th√¥ng tin
    chunk_overlap=100,  # TƒÉng overlap ƒë·ªÉ gi·ªØ context
    separators=[
        "\n### ",  # Ph√¢n chia theo section headers
        "\n## ",   # Headers nh·ªè h∆°n
        "\n- ",    # List items
        ".\n",     # K·∫øt th√∫c c√¢u + newline
        ". ",      # K·∫øt th√∫c c√¢u
        "\n",      # Newline
        " "        # Space (fallback)
    ]
)

# Check if the index exists, if not create it
if not pc.has_index(index_name):
    pc.create_index_for_model(
        name=index_name,
        cloud="aws",
        region="us-east-1",
        embed={
            "model":"llama-text-embed-v2",
            "field_map":{"text": "text"}
        }
    )

# Connect to the index
index = pc.Index(index_name)


# Define endpoints
@app.get("/")
def hello_world():
    return {"message": "Hello World!"}

@app.head("/v1/keep-alive")
def health_check():
        return {"status": "healthy"}

@app.post("/v1/ingest")
def ingest(payload: IngestPayload):
        
        # Parse JSON data t·ª´ backend
        import json
        try:
            destination_data = json.loads(payload.info)
        except:
            # Fallback n·∫øu v·∫´n l√† string c≈©
            destination_data = {"description": payload.info}
        
        # Semantic chunking theo lo·∫°i destination
        chunks = create_semantic_chunks(payload.name, destination_data, payload.destinationId, payload.slug)

        # T·∫°o records v·ªõi metadata ƒë·∫ßy ƒë·ªß
        records = [
            {
                "id": f"{payload.destinationId}-{chunk['type']}",
                "text": chunk['content'],
                'destinationId': payload.destinationId,
                'slug': payload.slug,
                'name': payload.name,
                'chunk_type': chunk['type'],
                'chunk_index': i,
                'total_chunks': len(chunks)
            } for i, chunk in enumerate(chunks)
        ]

        # Batch size of 90 (below Pinecone's limit of 96)
        batch_size = 90
    
        # Split records into batches and upsert
        for i in range(0, len(records), batch_size):
            batch = records[i:i + batch_size]
            index.upsert_records(payload.cityId, batch)

        return {"status": "done", "chunks_processed": len(records)}

@app.post("/v1/update")
def update_destination(payload: UpdatePayload):
    try:

        
        # B∆∞·ªõc 1: T√¨m v√† x√≥a c√°c chunks c≈© t·ª´ T·∫§T C·∫¢ namespaces
        # V√¨ c√≥ th·ªÉ cityId ƒë√£ thay ƒë·ªïi, ta c·∫ßn t√¨m trong t·∫•t c·∫£ namespaces
        deleted_count = 0
        
        # L·∫•y danh s√°ch t·∫•t c·∫£ namespaces
        try:
            stats = index.describe_index_stats()
            all_namespaces = list(stats.get('namespaces', {}).keys())
            
            # T√¨m v√† x√≥a chunks c≈© trong t·∫•t c·∫£ namespaces
            for namespace in all_namespaces:
                try:
                    ids_to_delete = list(index.list(prefix=payload.destinationId, namespace=namespace))
                    if ids_to_delete:
                        index.delete(namespace=namespace, ids=ids_to_delete)
                        deleted_count += len(ids_to_delete)
                except Exception as ns_error:
                    continue
                    
        except Exception as stats_error:
            # Fallback: ch·ªâ x√≥a t·ª´ namespace hi·ªán t·∫°i
            try:
                ids_to_delete = list(index.list(prefix=payload.destinationId, namespace=payload.cityId))
                if ids_to_delete:
                    index.delete(namespace=payload.cityId, ids=ids_to_delete)
                    deleted_count = len(ids_to_delete)
            except Exception as fallback_error:
                pass
        
        # B∆∞·ªõc 2: Parse JSON data t·ª´ backend (gi·ªëng nh∆∞ ingest)
        import json
        try:
            destination_data = json.loads(payload.info)
            print(f"[UPDATE] üìç ƒêang update ƒë·ªãa ƒëi·ªÉm: {payload.name}")
        except Exception as parse_error:
            destination_data = {"description": payload.info}
        
        # B∆∞·ªõc 3: T·∫°o semantic chunks m·ªõi v·ªõi 4 chunks
        chunks = create_semantic_chunks(payload.name, destination_data, payload.destinationId, payload.slug)

        # B∆∞·ªõc 4: T·∫°o records m·ªõi v·ªõi metadata ƒë·∫ßy ƒë·ªß
        records = [
            {
                "id": f"{payload.destinationId}-{chunk['type']}",
                "text": chunk['content'],
                'destinationId': payload.destinationId,
                'slug': payload.slug,
                'name': payload.name,
                'chunk_type': chunk['type'],
                'chunk_index': i,
                'total_chunks': len(chunks)
            } for i, chunk in enumerate(chunks)
        ]

        # B∆∞·ªõc 5: Upsert records m·ªõi v√†o namespace m·ªõi (cityId t·ª´ payload)
        batch_size = 90
        
        for i in range(0, len(records), batch_size):
            batch = records[i:i + batch_size]
            index.upsert_records(payload.cityId, batch)

        print(f"[UPDATE] ‚úÖ UPDATE TH√ÄNH C√îNG: {payload.name}")
        
        return {
            "status": "updated", 
            "chunks_deleted": deleted_count,
            "chunks_created": len(records),
            "new_namespace": payload.cityId,
            "destination_name": payload.name
        }
        
    except Exception as e:
        print(f"[UPDATE ERROR] ‚ùå UPDATE TH·∫§T B·∫†I cho {payload.name}: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Update failed: {str(e)}")

@app.post("/v1/question")
def question(payload: QuestionPayload):
    # Define the query

    # Search the dense index v·ªõi tƒÉng top_k ƒë·ªÉ bao ph·ªß t·ªët h∆°n
    results = index.search(
        namespace=payload.cityId,
        query={
            "top_k": 12,  # TƒÉng l√™n v√¨ m·ªói destination c√≥ 4 chunks
            "inputs": {
                'text': payload.query
            }
        }
    )

    # Print the results v·ªõi th√¥ng tin chunk type
    for hit in results['result']['hits']:
            chunk_type = hit['fields'].get('chunk_type', 'unknown')
            print(f"id: {hit['_id']:<5} | type: {chunk_type:<10} | destinationId: {hit['fields']['destinationId']} | text: {hit['fields']['text'][:50]}")
            

    chat_completion = client.chat.completions.create(
    messages=[
        {
            "role": "user",
            "content": (
                "### üìò Y√™u c·∫ßu:\n"
                f"Tr·∫£ l·ªùi c√¢u h·ªèi sau b·∫±ng c√°ch d·ª±a tr√™n c√°c ƒëo·∫°n vƒÉn b√™n d∆∞·ªõi. "
                "N·∫øu th√¥ng tin kh√¥ng ƒë·ªß, h√£y tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c c·ªßa b·∫°n v√† ghi r√µ ƒëi·ªÅu ƒë√≥.\n\n"
                f"**C√¢u h·ªèi:** {payload.query}\n\n"
                "### üìö ƒêo·∫°n vƒÉn tham kh·∫£o:\n"
                # + "\n---\n".join([hit['fields']['text'] for hit in results['result']['hits']]) +
                # "\n\n"
                + "\n---\n".join([
                     f"**ƒêo·∫°n vƒÉn {i+1}:**\n"
                     f"{hit['fields']['text']}\n"
                     for i, hit in enumerate(results['result']['hits'])
                     ]) +
                "### ‚úèÔ∏è Ghi ch√∫ khi tr·∫£ l·ªùi:\n"
                "- Tr√¨nh b√†y c√¢u tr·∫£ l·ªùi b·∫±ng [Markdown] ƒë·ªÉ h·ªá th·ªëng `react-markdown` c√≥ th·ªÉ hi·ªÉn th·ªã t·ªët.\n"
                "- Th√™m emoji ph√π h·ª£p ƒë·ªÉ l√†m n·ªïi b·∫≠t n·ªôi dung ch√≠nh üß†üìåüí°.\n"
                "- N·∫øu c√¢u tr·∫£ l·ªùi kh√¥ng th·ªÉ r√∫t ra t·ª´ ƒëo·∫°n vƒÉn, h√£y b·∫Øt ƒë·∫ßu b·∫±ng c√¢u: `D∆∞·ªõi ƒë√¢y l√† m·ªôt s·ªë g·ª£i √Ω c·ªßa t√¥i, ƒë·ªÉ c√≥ th·ªÉ nh·∫≠n g·ª£i √Ω ch√≠nh x√°c h∆°n t·ª´ h·ªá th·ªëng, vui l√≤ng ch·ªçn ƒëi·ªÉm ƒë·∫øn tr∆∞·ªõc nh√©`"
            )
        }
    ],
    model=payload.model or "deepseek-r1-distill-llama-70b",
    )
    response_dict = chat_completion.model_dump()
    response_dict["choices"][0]["message"]["documents"] = [
        {
            "id": hit["_id"],
            "text": hit["fields"]["text"],
            "destinationId": hit["fields"]["destinationId"],
            "score": hit["_score"]
        } for hit in results['result']['hits']
    ]
    return response_dict

@app.post("/v1/delete-document")
def delete_document(payload: DeletePayload):

    ids_to_delete = list(index.list(prefix=payload.destiationId, namespace=payload.cityId))

    if not ids_to_delete:
        raise HTTPException(status_code=404, detail="Kh√¥ng t√¨m th·∫•y vectors n√†o v·ªõi documentId n√†y.")

    # B∆∞·ªõc 2: Xo√° vector theo ID
    index.delete(
        namespace=payload.cityId,
        ids=ids_to_delete
    )

    return {"deleted_ids": ids_to_delete}

# H√†m clean_text ƒë·ªÉ x·ª≠ l√Ω vƒÉn b·∫£n
def clean_text(text: str) -> str:
    """
    L√†m s·∫°ch v√† chu·∫©n h√≥a vƒÉn b·∫£n ti·∫øng Vi·ªát
    """
    # 1. Chu·∫©n h√≥a Unicode (d√πng NFC ƒë·ªÉ gh√©p d·∫•u)
    text = unicodedata.normalize("NFC", text)

    # 2. Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát (gi·ªØ l·∫°i ti·∫øng Vi·ªát v√† ch·ªØ s·ªë)
    text = re.sub(r"[^\w\s√†√°·∫°·∫£√£√¢·∫ß·∫•·∫≠·∫©·∫´ƒÉ·∫±·∫Ø·∫∑·∫≥·∫µ√®√©·∫π·∫ª·∫Ω√™·ªÅ·∫ø·ªá·ªÉ·ªÖ√¨√≠·ªã·ªâƒ©"
                r"√≤√≥·ªç·ªè√µ√¥·ªì·ªë·ªô·ªï·ªó∆°·ªù·ªõ·ª£·ªü·ª°√π√∫·ª•·ªß≈©∆∞·ª´·ª©·ª±·ª≠·ªØ·ª≥√Ω·ªµ·ª∑·ªπƒë]", "", text)

    # 3. Lo·∫°i b·ªè kho·∫£ng tr·∫Øng d∆∞ th·ª´a
    text = re.sub(r"\s+", " ", text).strip()

    return text

def create_semantic_chunks(name: str, data: dict, destination_id: str, slug: str = None) -> list:
    """
    T·∫°o 4 chunks semantic cho m·ªói destination theo strategy m·ªõi
    """
    chunks = []
    
    # 1. T·ªïng quan - Th√¥ng tin t·ªïng quan (b·∫Øt ƒë·∫ßu b·∫±ng t√™n ƒë·ªãa ƒëi·ªÉm)
    overview_content = f"**{name}**\n\n"
    
    # M√¥ t·∫£ ch√≠nh
    if data.get('description'):
        overview_content += f"{data['description']}\n\n"
    
    # ƒêi·ªÉm n·ªïi b·∫≠t
    if data.get('highlight'):
        overview_content += f"**ƒêi·ªÉm n·ªïi b·∫≠t:** {data['highlight']}\n\n"
    
    chunks.append({
        'type': 'tong-quan',
        'content': overview_content.strip()
    })
    
    # 2. Tr·∫£i nghi·ªám - Tr·∫£i nghi·ªám v√† ho·∫°t ƒë·ªông (b·∫Øt ƒë·∫ßu b·∫±ng t√™n ƒë·ªãa ƒëi·ªÉm)
    experience_content = f"**{name}**\n\n"
    
    # D·ªãch v·ª•
    if data.get('services'):
        experience_content += f"**D·ªãch v·ª•:** {data['services']}\n\n"
    
    # Ho·∫°t ƒë·ªông
    if data.get('activities'):
        experience_content += f"**Ho·∫°t ƒë·ªông:** {data['activities']}\n\n"
    
    # Th√¥ng tin h·ªØu √≠ch
    if data.get('usefulInfo'):
        experience_content += f"**Th√¥ng tin h·ªØu √≠ch:** {data['usefulInfo']}\n\n"
    
    chunks.append({
        'type': 'trai-nghiem',
        'content': experience_content.strip()
    })
    
    # 3. Th·ª±c t·∫ø - Th√¥ng tin th·ª±c t·∫ø (b·∫Øt ƒë·∫ßu b·∫±ng t√™n ƒë·ªãa ƒëi·ªÉm)
    practical_content = f"**{name}**\n\n"
    
    # Gi·ªù m·ªü c·ª≠a
    if data.get('openHour'):
        practical_content += f"**Gi·ªù m·ªü c·ª≠a:** {data['openHour']}\n\n"
    
    # Ph√≠ tham quan
    if data.get('fee'):
        practical_content += f"**Ph√≠ tham quan:** {data['fee']}\n\n"
    
    # Th√¥ng tin li√™n h·ªá
    if data.get('contactInfo'):
        practical_content += f"**Li√™n h·ªá:** {data['contactInfo']}\n\n"
    
    chunks.append({
        'type': 'thuc-te',
        'content': practical_content.strip()
    })
    
    # 4. Danh m·ª•c - Tags v√† t·ª´ kh√≥a t√¨m ki·∫øm (b·∫Øt ƒë·∫ßu b·∫±ng t√™n ƒë·ªãa ƒëi·ªÉm)
    tags_content = f"**{name}**\n\n"
    
    # Tags ch√≠nh - l√†m n·ªïi b·∫≠t ƒë·ªÉ AI d·ªÖ nh·∫≠n bi·∫øt
    if data.get('tags'):
        tags_content += f"üè∑Ô∏è **DANH M·ª§C/TAGS:** {data['tags']}\n\n"
    
    # Lo·∫°i h√¨nh du l·ªãch - chuy·ªÉn t·ª´ chunk t·ªïng quan
    if data.get('cultureType'):
        tags_content += f"üéØ **LO·∫†I H√åNH DU L·ªäCH:** {data['cultureType']}\n\n"
    
    # Th√™m th√¥ng tin ph√¢n lo·∫°i ƒë·ªÉ AI d·ªÖ ph√¢n t√≠ch
    if data.get('type'):
        tags_content += f"üìÇ **PH√ÇN LO·∫†I:** {data['type']}\n\n"

    chunks.append({
        'type': 'danh-muc',
        'content': tags_content.strip()
    })
    
    return chunks

def filter_destinations_by_content(content: str, hits: list) -> list:
    """
    L·ªçc destinations d·ª±a tr√™n vi·ªác t√™n ƒë·ªãa ƒëi·ªÉm c√≥ xu·∫•t hi·ªán trong n·ªôi dung c√¢u tr·∫£ l·ªùi hay kh√¥ng
    S·ª≠ d·ª•ng multiple matching strategies ƒë·ªÉ tƒÉng ƒë·ªô ch√≠nh x√°c
    """
    if not content or not hits:
        return hits
    
    # Chu·∫©n h√≥a content ƒë·ªÉ so s√°nh
    content_normalized = clean_text(content.lower())
    
    # T·∫°o set ƒë·ªÉ tr√°nh tr√πng l·∫∑p destinations
    mentioned_destination_ids = set()
    filtered_hits = []
    
    for hit in hits:
        dest_name = hit["fields"].get("name", "").strip()
        dest_id = hit["fields"].get("destinationId")
        
        if not dest_name or not dest_id:
            continue
            
        # Chu·∫©n h√≥a t√™n ƒë·ªãa ƒëi·ªÉm ƒë·ªÉ so s√°nh
        dest_name_normalized = clean_text(dest_name.lower())
        
        # Strategy 1: Exact match
        is_mentioned = dest_name_normalized in content_normalized
        
        # Strategy 2: Partial word match (tr√°nh false positive)
        if not is_mentioned:
            # T√°ch t·ª´ v√† ki·ªÉm tra t·ª´ng t·ª´ quan tr·ªçng
            dest_words = [word for word in dest_name_normalized.split() if len(word) > 2]
            if dest_words:
                # Ph·∫£i c√≥ √≠t nh·∫•t 70% t·ª´ xu·∫•t hi·ªán trong content
                matched_words = sum(1 for word in dest_words if word in content_normalized)
                word_match_ratio = matched_words / len(dest_words)
                is_mentioned = word_match_ratio >= 0.7
        
        # Strategy 3: Common abbreviations v√† alternative names
        if not is_mentioned:
            # Ki·ªÉm tra c√°c pattern ph·ªï bi·∫øn
            name_patterns = [
                dest_name_normalized.replace(" ", ""),  # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng
                dest_name_normalized.replace("qu√°n", "").strip(),  # Lo·∫°i b·ªè "qu√°n"
                dest_name_normalized.replace("nh√† h√†ng", "").strip(),  # Lo·∫°i b·ªè "nh√† h√†ng"
                dest_name_normalized.replace("kh√°ch s·∫°n", "").strip(),  # Lo·∫°i b·ªè "kh√°ch s·∫°n"
                dest_name_normalized.replace("c√† ph√™", "coffee").strip(),  # Thay th·∫ø c√† ph√™
            ]
            
            for pattern in name_patterns:
                if pattern and len(pattern) > 2 and pattern in content_normalized:
                    is_mentioned = True
                    break
        
        if is_mentioned:
            # Ch·ªâ th√™m n·∫øu ch∆∞a c√≥ destination n√†y
            if dest_id not in mentioned_destination_ids:
                mentioned_destination_ids.add(dest_id)
                filtered_hits.append(hit)
                print(f"[FILTER] ‚úÖ Gi·ªØ l·∫°i: {dest_name} (xu·∫•t hi·ªán trong c√¢u tr·∫£ l·ªùi)")
            else:
                print(f"[FILTER] ‚ö†Ô∏è B·ªè qua duplicate: {dest_name}")
        else:
            print(f"[FILTER] ‚ùå Lo·∫°i b·ªè: {dest_name} (kh√¥ng xu·∫•t hi·ªán trong c√¢u tr·∫£ l·ªùi)")
    
    print(f"[FILTER] üìä K·∫øt qu·∫£: {len(filtered_hits)}/{len(hits)} destinations ƒë∆∞·ª£c gi·ªØ l·∫°i")
    return filtered_hits

@app.post("/v1/chat/completions")
def create_chat_completion(payload: ChatCompletionPayload):
    """
    Chat completion cho Gobot - tr·ª£ l√Ω du l·ªãch Vi·ªát Nam
    """
    # Th√¥ng b√°o chung khi ch∆∞a ch·ªçn th√†nh ph·ªë
    notice = (
        "üëã Xin ch√†o! Hi·ªán t·∫°i b·∫°n **ch∆∞a ch·ªçn th√†nh ph·ªë** ho·∫∑c ƒëi·ªÉm ƒë·∫øn c·ª• th·ªÉ.\n"
        "ƒê·ªÉ nh·∫≠n g·ª£i √Ω **ch√≠nh x√°c t·ª´ h·ªá th·ªëng**, h√£y ch·ªçn m·ªôt th√†nh ph·ªë tr∆∞·ªõc nh√©! üèôÔ∏è\n"
    )

    # -----------------------
    # 1Ô∏è‚É£ Kh√¥ng d√πng Knowledge
    # -----------------------
    if not payload.isUseKnowledge or not payload.cityId:
        messages_for_api = [msg.model_dump() for msg in payload.messages]
        user_question = messages_for_api[-1]["content"] if messages_for_api else ""

        system_prompt = (
            "B·∫°n l√† **Gobot**, tr·ª£ l√Ω du l·ªãch th√¥ng minh v√† th√¢n thi·ªán c·ªßa website **GoOhNo**, n·ªÅn t·∫£ng h·ªó tr·ª£ l√™n k·∫ø ho·∫°ch du l·ªãch Vi·ªát Nam.\n"
            "B·∫°n ƒë√≥ng vai tr√≤ nh∆∞ m·ªôt **h∆∞·ªõng d·∫´n vi√™n b·∫£n ƒë·ªãa**, tr√≤ chuy·ªán t·ª± nhi√™n v√† g·∫ßn g≈©i ƒë·ªÉ gi√∫p ng∆∞·ªùi d√πng kh√°m ph√° Vi·ªát Nam d·ªÖ d√†ng.\n"
            "Quy t·∫Øc quan tr·ªçng:\n"
            "1. Ch·ªâ t∆∞ v·∫•n v·ªÅ c√°c ƒë·ªãa ƒëi·ªÉm, ho·∫°t ƒë·ªông v√† tr·∫£i nghi·ªám du l·ªãch t·∫°i Vi·ªát Nam.\n"
            "2. Tr·∫£ l·ªùi b·∫±ng **ti·∫øng Vi·ªát**, gi·ªçng ƒëi·ªáu th√¢n thi·ªán, g·∫ßn g≈©i, d·ªÖ hi·ªÉu.\n"
            "3. Tr√¨nh b√†y b·∫±ng **Markdown** v·ªõi ti√™u ƒë·ªÅ, danh s√°ch v√† emoji minh h·ªça (üìçüèñÔ∏è‚òïüçúüèØ).\n"
            "4. N·∫øu kh√¥ng ch·∫Øc ch·∫Øn, h√£y n√≥i: *T√¥i kh√¥ng ch·∫Øc v·ªÅ ƒëi·ªÅu n√†y.*\n"
            "5. Cu·ªëi c√¢u tr·∫£ l·ªùi, th√™m **l·ªùi khuy√™n h·ªØu √≠ch cho du kh√°ch** v√† nh·∫Øc nh·∫π r·∫±ng h·ªç c√≥ th·ªÉ t√¨m hi·ªÉu th√™m tr√™n GoOhNo.\n"
        )


        user_prompt = (
            f"\"C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: {user_question}\"\n"
            "\"H√£y tr·∫£ l·ªùi d·ª±a tr√™n ki·∫øn th·ª©c n·ªÅn v·ªÅ du l·ªãch Vi·ªát Nam.\"\n"
            "\"ƒê∆∞a ra c√°c g·ª£i √Ω chi ti·∫øt, d·ªÖ ƒë·ªçc, k√®m emoji minh h·ªça.\"\n"
            "\"Chia nh·ªè n·ªôi dung th√†nh m·ª•c ho·∫∑c danh s√°ch Markdown.\"\n"
            "\"K·∫øt th√∫c b·∫±ng l·ªùi khuy√™n h·ªØu √≠ch v√† th√¢n thi·ªán.\"\n"
        )

        messages = [
            {"role": "system", "content": system_prompt},
            *messages_for_api,
            {"role": "user", "content": user_prompt},
        ]

        fallback_models = [
            payload.model or "llama-3.3-70b-versatile",
            "llama-3.1-70b-versatile",
            "mixtral-8x7b-32768"
        ]

        response_dict = None
        last_error = None

        for attempt, model in enumerate(fallback_models):
            try:
                print(f"[NO-KNOWLEDGE ATTEMPT {attempt+1}] Model: {model}")
                chat_completion = client.chat.completions.create(
                    messages=messages,
                    model=model,
                    temperature=0.4,
                    top_p=0.9,
                    max_completion_tokens=1024,
                )
                response_dict = chat_completion.model_dump()
                print(f"[SUCCESS] Model {model} worked!")
                break
            except Exception as e:
                last_error = e
                if attempt < len(fallback_models) - 1:
                    print(f"[FALLBACK] Error with {model}: {str(e)}")
                    continue

        if response_dict is None:
            raise HTTPException(status_code=500, detail=f"All fallback models failed. Last error: {str(last_error)}")

        # L√†m s·∫°ch n·ªôi dung tr·∫£ l·ªùi
        if response_dict.get("choices") and response_dict["choices"][0].get("message"):
            content = response_dict["choices"][0]["message"]["content"]
            import re
            content = re.sub(r'<thinking>.*?</thinking>', '', content, flags=re.DOTALL)
            content = re.sub(r'(Alright|First|I need|The user).*?(?=\n\n|\n[A-Z√Ä-·ª∏])', '', content, flags=re.DOTALL)
            content = re.sub(r'\n\s*\n\s*\n', '\n\n', content).strip()
            response_dict["choices"][0]["message"]["content"] = f"{notice}\n{content}"

        return response_dict

    # -----------------------
    # 2Ô∏è‚É£ D√πng Knowledge
    # -----------------------
    messages_for_api = [msg.model_dump() for msg in payload.messages]
    combined_question = clean_text(
        " ".join([msg.content for msg in payload.messages if msg.role == "user"])
    )

    results = index.search(
        namespace=payload.cityId,
        query={"top_k": 12, "inputs": {"text": combined_question}},  # TƒÉng top_k cho 4 chunks m·ªói destination
    )
    hits = results.get("result", {}).get("hits", [])

    # L·ªçc qu√°n c√† ph√™
    user_question = payload.messages[-1].content.lower()
    cafe_keywords = ["c√† ph√™", "coffee", "qu√°n cafe", "qu√°n c√† ph√™", "cafe 24h"]
    if any(kw in user_question for kw in cafe_keywords):
        filtered_hits = [
            hit for hit in hits
            if "cafe" in (hit["fields"].get("type","") + hit["fields"].get("category","")).lower()
            or "c√† ph√™" in (hit["fields"].get("type","") + hit["fields"].get("category","")).lower()
            or "coffee" in (hit["fields"].get("name","")).lower()
            or "c√† ph√™" in (hit["fields"].get("name","")).lower()
        ]
        if filtered_hits:
            hits = filtered_hits

    if not hits:
        return {
            "choices": [
                {"message": {"content": f"{notice}\n‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y d·ªØ li·ªáu ph√π h·ª£p."}}
            ]
        }

    # -----------------------
    # 3Ô∏è‚É£ Chu·∫©n b·ªã prompt
    # -----------------------
    # Tr√≠ch xu·∫•t t√™n ƒë·ªãa ƒëi·ªÉm t·ª´ hits
    destination_names = []
    reference_texts_with_names = []
    
    for hit in hits:
        dest_name = hit["fields"].get("name", "")
        if dest_name and dest_name not in destination_names:
            destination_names.append(dest_name)
        
        # ƒê·∫£m b·∫£o text lu√¥n c√≥ t√™n ƒë·ªãa ƒëi·ªÉm
        text = hit["fields"]["text"]
        if dest_name and dest_name not in text:
            text = f"**{dest_name}**: {text}"
        reference_texts_with_names.append(text)
    
    reference_texts = "\n---\n".join(reference_texts_with_names)

    system_prompt = (
        "B·∫°n l√† **Gobot**, tr·ª£ l√Ω du l·ªãch Vi·ªát Nam th√¢n thi·ªán v√† hi·ªÉu bi·∫øt üáªüá≥.\n"
        "Quy t·∫Øc:\n"
        "1. Ch·ªâ t∆∞ v·∫•n c√°c ƒë·ªãa ƒëi·ªÉm v√† tr·∫£i nghi·ªám t·∫°i Vi·ªát Nam.\n"
        "2. Tr·∫£ l·ªùi b·∫±ng ti·∫øng Vi·ªát, gi·ªçng ƒëi·ªáu t·ª± nhi√™n, d·ªÖ g·∫ßn, nh∆∞ ƒëang tr√≤ chuy·ªán.\n"
        "3. Tr√¨nh b√†y b·∫±ng **Markdown** v·ªõi ti√™u ƒë·ªÅ, danh s√°ch v√† emoji (üìç‚òïüèñÔ∏èüçúüèØ).\n"
        "4. **QUAN TR·ªåNG**: Lu√¥n s·ª≠ d·ª•ng T√äN CH√çNH X√ÅC c·ªßa ƒë·ªãa ƒëi·ªÉm t·ª´ d·ªØ li·ªáu ƒë∆∞·ª£c cung c·∫•p.\n"
        "5. **PH√ÇN T√çCH TAGS**: ƒê·ªçc k·ªπ tr∆∞·ªùng 'Danh m·ª•c' v√† 'Lo·∫°i h√¨nh' c·ªßa m·ªói ƒë·ªãa ƒëi·ªÉm ƒë·ªÉ ƒë√°nh gi√° m·ª©c ƒë·ªô ph√π h·ª£p v·ªõi y√™u c·∫ßu.\n"
        "6. N·∫øu kh√¥ng ch·∫Øc ch·∫Øn, h√£y n√≥i: *T√¥i kh√¥ng ch·∫Øc v·ªÅ ƒëi·ªÅu n√†y.*\n"
        "7. K·∫øt th√∫c c√¢u tr·∫£ l·ªùi b·∫±ng **l·ªùi khuy√™n h·ªØu √≠ch cho kh√°ch du l·ªãch**.\n"
    )

    user_prompt = (
        f"\"C√¢u h·ªèi c·ªßa ng∆∞·ªùi d√πng: {payload.messages[-1].content}\"\n"
        "\"D·ª±a tr√™n c√°c th√¥ng tin tham kh·∫£o t·ª´ h·ªá th·ªëng, h√£y tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß v√† th√¢n thi·ªán.\"\n"
        "\n**H∆Ø·ªöNG D·∫™N PH√ÇN T√çCH QUAN TR·ªåNG:**\n"
        "1. **ƒê·ªçc k·ªπ tags v√† danh m·ª•c**: Xem x√©t tr∆∞·ªùng 'Danh m·ª•c' v√† 'Lo·∫°i h√¨nh' c·ªßa m·ªói ƒë·ªãa ƒëi·ªÉm\n"
        "2. **ƒê√°nh gi√° ƒë·ªô ph√π h·ª£p**: Ch·ªâ ƒë·ªÅ xu·∫•t ƒë·ªãa ƒëi·ªÉm c√≥ tags/danh m·ª•c ph√π h·ª£p v·ªõi y√™u c·∫ßu\n"
        "3. **∆Øu ti√™n theo m·ª©c ƒë·ªô ph√π h·ª£p**: S·∫Øp x·∫øp ƒë·ªãa ƒëi·ªÉm theo ƒë·ªô ph√π h·ª£p t·ª´ cao ƒë·∫øn th·∫•p\n"
        "4. **Gi·∫£i th√≠ch l√Ω do**: N√™u r√µ t·∫°i sao ƒë·ªãa ƒëi·ªÉm ph√π h·ª£p d·ª±a tr√™n tags/danh m·ª•c\n"
        "\n**V√ç D·ª§ PH√ÇN T√çCH:**\n"
        "- N·∫øu user h·ªèi v·ªÅ 'qu√°n c√† ph√™': Ch·ªâ ch·ªçn ƒë·ªãa ƒëi·ªÉm c√≥ tag 'c√† ph√™', 'coffee', 'ƒë·ªì u·ªëng'\n"
        "- N·∫øu user h·ªèi v·ªÅ 'ƒÉn u·ªëng': Ch·ªçn ƒë·ªãa ƒëi·ªÉm c√≥ tag '·∫©m th·ª±c', 'nh√† h√†ng', 'm√≥n ƒÉn'\n"
        "- N·∫øu user h·ªèi v·ªÅ 'du l·ªãch vƒÉn h√≥a': Ch·ªçn ƒë·ªãa ƒëi·ªÉm c√≥ tag 'vƒÉn h√≥a', 'l·ªãch s·ª≠', 'truy·ªÅn th·ªëng'\n"
        "\n"
        "\"""QUAN TR·ªåNG: H√£y ki·ªÉm tra th·ªùi gian m·ªü c·ª≠a c·ªßa ƒë·ªãa ƒëi·ªÉm tr∆∞·ªõc khi tr·∫£ l·ªùi, n·∫øu ƒë·ªãa ƒëi·ªÉm ƒë√£ ƒë√≥ng c·ª≠a th√¨ lo·∫°i ƒë·ªãa ƒëi·ªÉm ƒë√≥ ra kh·ªèi c√¢u tr·∫£ l·ªùi.\"\n"
        f"\"C√°c ƒë·ªãa ƒëi·ªÉm c√≥ s·∫µn: {', '.join(destination_names)}\"\n"
        "\"Th√¥ng tin chi ti·∫øt:\" \n"
        f"{reference_texts}\n\n"
        "\"QUAN TR·ªåNG: \"\n"
        "\"- H√£y s·ª≠ d·ª•ng CH√çNH X√ÅC t√™n ƒë·ªãa ƒëi·ªÉm t·ª´ danh s√°ch tr√™n.\"\n"
        "\"- CH·ªà ƒë∆∞a v√†o c√¢u tr·∫£ l·ªùi nh·ªØng ƒë·ªãa ƒëi·ªÉm c√≥ tags/danh m·ª•c PH√ô H·ª¢P v·ªõi y√™u c·∫ßu c·ªßa user.\"\n"
        "\"- Tr√¨nh b√†y d∆∞·ªõi d·∫°ng danh s√°ch Markdown v·ªõi emoji, c√≥ t√™n ƒë·ªãa ƒëi·ªÉm r√µ r√†ng.\"\n"
        "\"- Gi·∫£i th√≠ch ng·∫Øn g·ªçn t·∫°i sao ƒë·ªãa ƒëi·ªÉm ph√π h·ª£p (d·ª±a tr√™n tags/danh m·ª•c).\"\n"
        "\"- K·∫øt th√∫c b·∫±ng m·ªôt l·ªùi khuy√™n h·ªØu √≠ch v√† th√¢n thi·ªán.\"\n"
        "\n**QUY TR√åNH PH√ÇN T√çCH TAGS:**\n"
        "\"1. ƒê·ªçc t·ª´ng ƒë·ªãa ƒëi·ªÉm v√† t√¨m ph·∫ßn c√≥ emoji üè∑Ô∏è DANH M·ª§C/TAGS v√† üéØ LO·∫†I H√åNH DU L·ªäCH\"\n"
        "\"2. So s√°nh tags v·ªõi t·ª´ kh√≥a trong c√¢u h·ªèi c·ªßa user (v√≠ d·ª•: 'c√† ph√™' kh·ªõp v·ªõi tag 'coffee')\"\n"
        "\"3. Ch·ªâ ƒë∆∞a v√†o c√¢u tr·∫£ l·ªùi nh·ªØng ƒë·ªãa ƒëi·ªÉm c√≥ tags ph√π h·ª£p >= 70%\"\n"
        "\"4. S·∫Øp x·∫øp theo m·ª©c ƒë·ªô ph√π h·ª£p: R·∫•t ph√π h·ª£p > Ph√π h·ª£p > C√≥ th·ªÉ ph√π h·ª£p\"\n"
        "\"5. Trong c√¢u tr·∫£ l·ªùi, ghi r√µ l√Ω do ch·ªçn d·ª±a tr√™n tags (v√≠ d·ª•: 'ph√π h·ª£p v·ªõi nhu c·∫ßu t√¨m c√† ph√™')\"\n"
    )

    messages = [
        {"role": "system", "content": system_prompt},
        *messages_for_api,
        {"role": "user", "content": user_prompt},
    ]

    fallback_models = [
        payload.model or "deepseek-r1-distill-llama-70b",
        "llama-3.3-70b-versatile",
        "llama-3.1-70b-versatile",
        "mixtral-8x7b-32768"
    ]

    response_dict = None
    last_error = None

    for attempt, model in enumerate(fallback_models):
        try:
            print(f"[ATTEMPT {attempt+1}] Model: {model}")
            chat_completion = client.chat.completions.create(
                messages=messages,
                model=model,
                temperature=0.2,
                top_p=0.85,
                max_completion_tokens=800,
            )
            response_dict = chat_completion.model_dump()
            print(f"[SUCCESS] Model {model} worked!")
            break
        except Exception as e:
            last_error = e
            print(f"[ERROR] {model}: {str(e)}")
            if attempt < len(fallback_models) - 1:
                continue

    if response_dict is None:
        raise HTTPException(status_code=500, detail=f"All models failed. Last error: {str(last_error)}")

    # L√†m s·∫°ch output v√† l·ªçc destinations d·ª±a tr√™n n·ªôi dung c√¢u tr·∫£ l·ªùi
    if response_dict.get("choices") and response_dict["choices"][-1].get("message"):
        content = response_dict["choices"][-1]["message"]["content"]
        import re
        content = re.sub(r'\n\s*\n\s*\n', '\n\n', content).strip()
        response_dict["choices"][-1]["message"]["content"] = content
        
        # L·ªçc destinations ch·ªâ gi·ªØ l·∫°i nh·ªØng ƒë·ªãa ƒëi·ªÉm ƒë∆∞·ª£c nh·∫Øc ƒë·∫øn trong c√¢u tr·∫£ l·ªùi
        filtered_hits = filter_destinations_by_content(content, hits)
    else:
        # Fallback n·∫øu kh√¥ng c√≥ content
        filtered_hits = hits

    response_dict["choices"][-1]["message"]["destinations"] = [
        {
            "id": hit.get("_id") or hit.get("id", ""),
            "text": hit["fields"]["text"],
            "destinationId": hit["fields"].get("destinationId"),
            "slug": hit["fields"].get("slug"),
            "name": hit["fields"].get("name"),
            "chunk_type": hit["fields"].get("chunk_type", "unknown"),
            "score": hit.get("_score", 0),
        } for hit in filtered_hits
    ]

    return response_dict
